{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import json\n",
    "import seaborn as sns\n",
    "import os\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "import plotly.express as px\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, Dataset, DataLoader, random_split\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import lr_scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config:\n",
    "    train_file = 'dataset/train.json'\n",
    "    test_file = 'dataset/test.json'\n",
    "    pretrain_dir = 'models'\n",
    "    sample_submission = 'dataset/sample_submission.csv'\n",
    "    learning_rate = 0.001\n",
    "    batch_size = 64\n",
    "    n_epoch = 100\n",
    "    n_split = 5\n",
    "    K = 1 # number of aggregation loop (also means number of GCN layers)\n",
    "    gcn_agg = 'mean' # aggregator function: mean, conv, lstm, pooling\n",
    "    filter_noise = True\n",
    "    seed = 1234"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter:\n",
    "    \"\"\"\n",
    "    Computes and stores the average and current value\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=1234):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "seed_everything(config.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    '''\n",
    "    Implementation of one layer of GraphSAGE\n",
    "    '''\n",
    "    def __init__(self, input_dim, output_dim, aggregator='mean'):\n",
    "        super(GCN, self).__init__()\n",
    "        self.aggregator = aggregator\n",
    "        \n",
    "        if aggregator == 'mean':\n",
    "            linear_input_dim = input_dim * 2\n",
    "        elif aggregator == 'conv':\n",
    "            linear_input_dim = input_dim\n",
    "        elif aggregator == 'pooling':\n",
    "            linear_input_dim = input_dim * 2\n",
    "            self.linear_pooling = nn.Linear(input_dim, input_dim)\n",
    "        elif aggregator == 'lstm':\n",
    "            self.lstm_hidden = 128\n",
    "            linear_input_dim = input_dim + self.lstm_hidden\n",
    "            self.lstm_agg = nn.LSTM(input_dim, self.lstm_hidden, num_layers=1, batch_first=True)\n",
    "        \n",
    "        self.linear_gcn = nn.Linear(in_features=linear_input_dim, out_features=output_dim)\n",
    "        \n",
    "    def forward(self, input_, adj_matrix):\n",
    "        if self.aggregator == 'conv':\n",
    "            # set elements in diagonal of adj matrix to 1 with conv aggregator\n",
    "            idx = torch.arange(0, adj_matrix.shape[-1], out=torch.LongTensor())\n",
    "            adj_matrix[:, idx, idx] = 1\n",
    "            \n",
    "        adj_matrix = adj_matrix.type(torch.float32)\n",
    "        sum_adj = torch.sum(adj_matrix, axis=2)\n",
    "        sum_adj[sum_adj==0] = 1\n",
    "        \n",
    "        if self.aggregator == 'mean' or self.aggregator == 'conv':\n",
    "            feature_agg = torch.bmm(adj_matrix, input_)\n",
    "            feature_agg = feature_agg / sum_adj.unsqueeze(dim=2)\n",
    "            \n",
    "        elif self.aggregator == 'pooling':\n",
    "            feature_pooling = self.linear_pooling(input_)\n",
    "            feature_agg = torch.sigmoid(feature_pooling)\n",
    "            feature_agg = torch.bmm(adj_matrix, feature_agg)\n",
    "            feature_agg = feature_agg / sum_adj.unsqueeze(dim=2)\n",
    "\n",
    "        elif self.aggregator == 'lstm':\n",
    "            feature_agg = torch.zeros(input_.shape[0], input_.shape[1], self.lstm_hidden).cuda()\n",
    "            for i in range(adj_matrix.shape[1]):\n",
    "                neighbors = adj_matrix[:, i, :].unsqueeze(2) * input_\n",
    "                _, hn = self.lstm_agg(neighbors)\n",
    "                feature_agg[:, i, :] = torch.squeeze(hn[0], 0)\n",
    "                \n",
    "        if self.aggregator != 'conv':\n",
    "            feature_cat = torch.cat((input_, feature_agg), axis=2)\n",
    "        else:\n",
    "            feature_cat = feature_agg\n",
    "                \n",
    "        feature = torch.sigmoid(self.linear_gcn(feature_cat))\n",
    "        feature = feature / torch.norm(feature, p=2, dim=2).unsqueeze(dim=2)\n",
    "        \n",
    "        return feature\n",
    "        \n",
    "    \n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_embedding=14, seq_len=107, pred_len=68, dropout=0.5, \n",
    "                 embed_dim=100, hidden_dim=128, K=1, aggregator='mean'):\n",
    "        '''\n",
    "        K: number of GCN layers\n",
    "        aggregator: type of aggregator function\n",
    "        '''\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.pred_len = pred_len\n",
    "        self.embedding_layer = nn.Embedding(num_embeddings=num_embedding, \n",
    "                                      embedding_dim=embed_dim)\n",
    "        \n",
    "        self.gcn = nn.ModuleList([GCN(3 * embed_dim, 3 * embed_dim, aggregator=aggregator) for i in range(K)])\n",
    "        \n",
    "        self.gru_layer = nn.GRU(input_size=3 * embed_dim, \n",
    "                          hidden_size=hidden_dim, \n",
    "                          num_layers=3, \n",
    "                          batch_first=True, \n",
    "                          dropout=dropout, \n",
    "                          bidirectional=True)\n",
    "        \n",
    "        self.linear_layer = nn.Linear(in_features=2 * hidden_dim, \n",
    "                                out_features=5)\n",
    "        \n",
    "    def forward(self, input_, adj_matrix):\n",
    "        #embedding\n",
    "        embedding = self.embedding_layer(input_)\n",
    "        embedding = torch.reshape(embedding, (-1, embedding.shape[1], embedding.shape[2] * embedding.shape[3]))\n",
    "        \n",
    "        #gcn\n",
    "        gcn_feature = embedding\n",
    "        for gcn_layer in self.gcn:\n",
    "            gcn_feature = gcn_layer(gcn_feature, adj_matrix)\n",
    "        \n",
    "        #gru\n",
    "        gru_output, gru_hidden = self.gru_layer(gcn_feature)\n",
    "        truncated = gru_output[:, :self.pred_len]\n",
    "        \n",
    "        output = self.linear_layer(truncated)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_cols = ['reactivity', 'deg_Mg_pH10', 'deg_pH10', 'deg_Mg_50C', 'deg_50C']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "token2int = {x:i for i, x in enumerate('().ACGUBEHIMSX')}\n",
    "\n",
    "def get_couples(structure):\n",
    "    \"\"\"\n",
    "    For each closing parenthesis, I find the matching opening one and store their index in the couples list.\n",
    "    The assigned list is used to keep track of the assigned opening parenthesis\n",
    "    \"\"\"\n",
    "    opened = [idx for idx, i in enumerate(structure) if i == '(']\n",
    "    closed = [idx for idx, i in enumerate(structure) if i == ')']\n",
    "\n",
    "    assert len(opened) == len(closed)\n",
    "    assigned = []\n",
    "    couples = []\n",
    "\n",
    "    for close_idx in closed:\n",
    "        for open_idx in opened:\n",
    "            if open_idx < close_idx:\n",
    "                if open_idx not in assigned:\n",
    "                    candidate = open_idx\n",
    "            else:\n",
    "                break\n",
    "        assigned.append(candidate)\n",
    "        couples.append([candidate, close_idx])\n",
    "        \n",
    "    assert len(couples) == len(opened)\n",
    "    \n",
    "    return couples\n",
    "\n",
    "def build_matrix(couples, size):\n",
    "    mat = np.zeros((size, size))\n",
    "    \n",
    "    for i in range(size):  # neigbouring bases are linked as well\n",
    "        if i < size - 1:\n",
    "            mat[i, i + 1] = 1\n",
    "        if i > 0:\n",
    "            mat[i, i - 1] = 1\n",
    "    \n",
    "    for i, j in couples:\n",
    "        mat[i, j] = 1\n",
    "        mat[j, i] = 1\n",
    "        \n",
    "    return mat\n",
    "\n",
    "def convert_to_adj(structure):\n",
    "    couples = get_couples(structure)\n",
    "    mat = build_matrix(couples, len(structure))\n",
    "    return mat\n",
    "\n",
    "def preprocess_inputs(df, cols=['sequence', 'structure', 'predicted_loop_type']):\n",
    "    inputs = np.transpose(\n",
    "        np.array(\n",
    "            df[cols]\n",
    "            .applymap(lambda seq: [token2int[x] for x in seq])\n",
    "            .values\n",
    "            .tolist()\n",
    "        ),\n",
    "        (0, 2, 1)\n",
    "    )\n",
    "    \n",
    "    adj_matrix = np.array(df['structure'].apply(convert_to_adj).values.tolist())\n",
    "    \n",
    "    return inputs, adj_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_json(config.train_file, lines=True)\n",
    "\n",
    "if config.filter_noise:\n",
    "    train = train[train.signal_to_noise > 1]\n",
    "    \n",
    "test = pd.read_json(config.test_file, lines=True)\n",
    "sample_df = pd.read_csv(config.sample_submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs, train_adj = preprocess_inputs(train)\n",
    "train_labels = np.array(train[pred_cols].values.tolist()).transpose((0, 2, 1))\n",
    "\n",
    "train_inputs = torch.tensor(train_inputs, dtype=torch.long)\n",
    "train_adj = torch.tensor(train_adj, dtype=torch.long)\n",
    "train_labels = torch.tensor(train_labels, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(epoch, model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    model.zero_grad()\n",
    "    train_loss = AverageMeter()\n",
    "    \n",
    "    for index, (input_, adj, label) in enumerate(train_loader):\n",
    "        input_ = input_.cuda()\n",
    "        adj = adj.cuda()\n",
    "        label = label.cuda()\n",
    "        preds = model(input_, adj)\n",
    "        \n",
    "        loss = criterion(preds, label)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss.update(loss.item())\n",
    "    \n",
    "    print(f\"Train loss {train_loss.avg}\")\n",
    "    return train_loss.avg\n",
    "    \n",
    "def eval_fn(epoch, model, valid_loader, criterion):\n",
    "    model.eval()\n",
    "    eval_loss = AverageMeter()\n",
    "    \n",
    "    for index, (input_, adj, label) in enumerate(valid_loader):\n",
    "        input_ = input_.cuda()\n",
    "        adj = adj.cuda()\n",
    "        label = label.cuda()\n",
    "        preds = model(input_, adj)\n",
    "        \n",
    "        loss = criterion(preds, label)\n",
    "        eval_loss.update(loss.item())\n",
    "    \n",
    "    print(f\"Valid loss {eval_loss.avg}\")\n",
    "    return eval_loss.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(fold, train_loader, valid_loader):\n",
    "    model = Net(K=config.K, aggregator=config.gcn_agg)\n",
    "    model.cuda()\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=config.learning_rate, weight_decay=0.0)\n",
    "    \n",
    "    train_losses = []\n",
    "    eval_losses = []\n",
    "    for epoch in range(config.n_epoch):\n",
    "        print('#################')\n",
    "        print('###Epoch:', epoch)\n",
    "        \n",
    "        train_loss = train_fn(epoch, model, train_loader, criterion, optimizer)\n",
    "        eval_loss = eval_fn(epoch, model, valid_loader, criterion)\n",
    "        train_losses.append(train_loss)\n",
    "        eval_losses.append(eval_loss)\n",
    "        \n",
    "    torch.save(model.state_dict(), f'{config.pretrain_dir}/005_gcn_gru_{fold}.pt')\n",
    "    return train_losses, eval_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################\n",
      "###Epoch: 0\n",
      "Train loss 0.2430855565600925\n",
      "Valid loss 0.20308280416897365\n",
      "#################\n",
      "###Epoch: 1\n",
      "Train loss 0.17902433817033414\n",
      "Valid loss 0.1762175645147051\n",
      "#################\n",
      "###Epoch: 2\n",
      "Train loss 0.15814734609038741\n",
      "Valid loss 0.15929346638066427\n",
      "#################\n",
      "###Epoch: 3\n",
      "Train loss 0.14225635760360295\n",
      "Valid loss 0.14066152274608612\n",
      "#################\n",
      "###Epoch: 4\n",
      "Train loss 0.13060493380935104\n",
      "Valid loss 0.13258612794535501\n",
      "#################\n",
      "###Epoch: 5\n",
      "Train loss 0.12359178314606349\n",
      "Valid loss 0.1290477386542729\n",
      "#################\n",
      "###Epoch: 6\n",
      "Train loss 0.11932558180005462\n",
      "Valid loss 0.1251306469951357\n",
      "#################\n",
      "###Epoch: 7\n",
      "Train loss 0.11652356193021492\n",
      "Valid loss 0.12640750514609472\n",
      "#################\n",
      "###Epoch: 8\n",
      "Train loss 0.11402010393363458\n",
      "Valid loss 0.1208049591098513\n",
      "#################\n",
      "###Epoch: 9\n",
      "Train loss 0.11131285296546088\n",
      "Valid loss 0.1183921652180808\n",
      "#################\n",
      "###Epoch: 10\n",
      "Train loss 0.10794003914903712\n",
      "Valid loss 0.11550723548446383\n",
      "#################\n",
      "###Epoch: 11\n",
      "Train loss 0.106015184963191\n",
      "Valid loss 0.1120440566114017\n",
      "#################\n",
      "###Epoch: 12\n",
      "Train loss 0.103014696251463\n",
      "Valid loss 0.11120592909199851\n",
      "#################\n",
      "###Epoch: 13\n",
      "Train loss 0.1010241820304482\n",
      "Valid loss 0.10581735521554947\n",
      "#################\n",
      "###Epoch: 14\n",
      "Train loss 0.09591465250209526\n",
      "Valid loss 0.10300129758460182\n",
      "#################\n",
      "###Epoch: 15\n",
      "Train loss 0.09426557879756998\n",
      "Valid loss 0.10050403539623533\n",
      "#################\n",
      "###Epoch: 16\n",
      "Train loss 0.09231365692836267\n",
      "Valid loss 0.09899215293782097\n",
      "#################\n",
      "###Epoch: 17\n",
      "Train loss 0.08983363404318138\n",
      "Valid loss 0.0966007432767323\n",
      "#################\n",
      "###Epoch: 18\n",
      "Train loss 0.08957533179609864\n",
      "Valid loss 0.09623844070093972\n",
      "#################\n",
      "###Epoch: 19\n",
      "Train loss 0.08756215070132856\n",
      "Valid loss 0.09421605510371071\n",
      "#################\n",
      "###Epoch: 20\n",
      "Train loss 0.0842831082366131\n",
      "Valid loss 0.09497733520609992\n",
      "#################\n",
      "###Epoch: 21\n",
      "Train loss 0.0839064973923895\n",
      "Valid loss 0.09292543147291456\n",
      "#################\n",
      "###Epoch: 22\n",
      "Train loss 0.08237027790811327\n",
      "Valid loss 0.09094817084925515\n",
      "#################\n",
      "###Epoch: 23\n",
      "Train loss 0.08011964956919353\n",
      "Valid loss 0.08693325838872365\n",
      "#################\n",
      "###Epoch: 24\n",
      "Train loss 0.07824409201189324\n",
      "Valid loss 0.08542031369038991\n",
      "#################\n",
      "###Epoch: 25\n",
      "Train loss 0.07718000326443601\n",
      "Valid loss 0.08431174499647957\n",
      "#################\n",
      "###Epoch: 26\n",
      "Train loss 0.07719350495824108\n",
      "Valid loss 0.08343580684491567\n",
      "#################\n",
      "###Epoch: 27\n",
      "Train loss 0.07388940274163529\n",
      "Valid loss 0.08261471880333764\n",
      "#################\n",
      "###Epoch: 28\n",
      "Train loss 0.072694749329929\n",
      "Valid loss 0.08213634150368827\n",
      "#################\n",
      "###Epoch: 29\n",
      "Train loss 0.07244393367458273\n",
      "Valid loss 0.0809392939720835\n",
      "#################\n",
      "###Epoch: 30\n",
      "Train loss 0.07198940769389824\n",
      "Valid loss 0.08114970688308988\n",
      "#################\n",
      "###Epoch: 31\n",
      "Train loss 0.07142788916826248\n",
      "Valid loss 0.0806317265544619\n",
      "#################\n",
      "###Epoch: 32\n",
      "Train loss 0.06959978187525714\n",
      "Valid loss 0.07791556205068316\n",
      "#################\n",
      "###Epoch: 33\n",
      "Train loss 0.06805208573738734\n",
      "Valid loss 0.07761913537979126\n",
      "#################\n",
      "###Epoch: 34\n",
      "Train loss 0.06785699311229917\n",
      "Valid loss 0.07739455785070147\n",
      "#################\n",
      "###Epoch: 35\n",
      "Train loss 0.06775790563336125\n",
      "Valid loss 0.07658975890704564\n",
      "#################\n",
      "###Epoch: 36\n",
      "Train loss 0.06672286311233486\n",
      "Valid loss 0.07635968391384397\n",
      "#################\n",
      "###Epoch: 37\n",
      "Train loss 0.06610574214546769\n",
      "Valid loss 0.07648508357150215\n",
      "#################\n",
      "###Epoch: 38\n",
      "Train loss 0.06488851678592188\n",
      "Valid loss 0.07692247416291918\n",
      "#################\n",
      "###Epoch: 39\n",
      "Train loss 0.06536415053738488\n",
      "Valid loss 0.07659695084605898\n",
      "#################\n",
      "###Epoch: 40\n",
      "Train loss 0.06386096582368568\n",
      "Valid loss 0.07459272337811333\n",
      "#################\n",
      "###Epoch: 41\n",
      "Train loss 0.06291139705313577\n",
      "Valid loss 0.07348616527659553\n",
      "#################\n",
      "###Epoch: 42\n",
      "Train loss 0.061906387408574425\n",
      "Valid loss 0.07326156326702662\n",
      "#################\n",
      "###Epoch: 43\n",
      "Train loss 0.06128881416387028\n",
      "Valid loss 0.07466288017375129\n",
      "#################\n",
      "###Epoch: 44\n",
      "Train loss 0.06180895989139875\n",
      "Valid loss 0.07351604742663247\n",
      "#################\n",
      "###Epoch: 45\n",
      "Train loss 0.060534856661602306\n",
      "Valid loss 0.07371645101479121\n",
      "#################\n",
      "###Epoch: 46\n",
      "Train loss 0.060654334448002\n",
      "Valid loss 0.07314727348940712\n",
      "#################\n",
      "###Epoch: 47\n",
      "Train loss 0.06010017668207487\n",
      "Valid loss 0.07287590418543134\n",
      "#################\n",
      "###Epoch: 48\n",
      "Train loss 0.05907092008877684\n",
      "Valid loss 0.07266378722020558\n",
      "#################\n",
      "###Epoch: 49\n",
      "Train loss 0.05893488711229077\n",
      "Valid loss 0.07147983089089394\n",
      "#################\n",
      "###Epoch: 50\n",
      "Train loss 0.058372269212095824\n",
      "Valid loss 0.0736797781927245\n",
      "#################\n",
      "###Epoch: 51\n",
      "Train loss 0.058312263891652776\n",
      "Valid loss 0.07199507632425853\n",
      "#################\n",
      "###Epoch: 52\n",
      "Train loss 0.0574885139034854\n",
      "Valid loss 0.07205570329512868\n",
      "#################\n",
      "###Epoch: 53\n",
      "Train loss 0.05720940977334976\n",
      "Valid loss 0.07143666808094297\n",
      "#################\n",
      "###Epoch: 54\n",
      "Train loss 0.05632805796685042\n",
      "Valid loss 0.07154928679977145\n",
      "#################\n",
      "###Epoch: 55\n",
      "Train loss 0.05619831245254587\n",
      "Valid loss 0.07144381425210408\n",
      "#################\n",
      "###Epoch: 56\n",
      "Train loss 0.056550339968116196\n",
      "Valid loss 0.0711461396089622\n",
      "#################\n",
      "###Epoch: 57\n",
      "Train loss 0.055728756443217946\n",
      "Valid loss 0.07255688416106361\n",
      "#################\n",
      "###Epoch: 58\n",
      "Train loss 0.0554273872187844\n",
      "Valid loss 0.071409030684403\n",
      "#################\n",
      "###Epoch: 59\n",
      "Train loss 0.054793293967291164\n",
      "Valid loss 0.07221231290272304\n",
      "#################\n",
      "###Epoch: 60\n",
      "Train loss 0.054897123050910455\n",
      "Valid loss 0.07570847017424447\n",
      "#################\n",
      "###Epoch: 61\n",
      "Train loss 0.05451733894922115\n",
      "Valid loss 0.07111345444406782\n",
      "#################\n",
      "###Epoch: 62\n",
      "Train loss 0.05354855692497006\n",
      "Valid loss 0.07134832601462092\n",
      "#################\n",
      "###Epoch: 63\n",
      "Train loss 0.05323640132943789\n",
      "Valid loss 0.06948053996477808\n",
      "#################\n",
      "###Epoch: 64\n",
      "Train loss 0.052924492982802565\n",
      "Valid loss 0.0698652198272092\n",
      "#################\n",
      "###Epoch: 65\n",
      "Train loss 0.05357237311976927\n",
      "Valid loss 0.07013187131711415\n",
      "#################\n",
      "###Epoch: 66\n",
      "Train loss 0.0520073008482103\n",
      "Valid loss 0.06925752865416664\n",
      "#################\n",
      "###Epoch: 67\n",
      "Train loss 0.0510373547397278\n",
      "Valid loss 0.06944718371544566\n",
      "#################\n",
      "###Epoch: 68\n",
      "Train loss 0.05142357793671114\n",
      "Valid loss 0.06928931442754609\n",
      "#################\n",
      "###Epoch: 69\n",
      "Train loss 0.051214497122499675\n",
      "Valid loss 0.06888756901025772\n",
      "#################\n",
      "###Epoch: 70\n",
      "Train loss 0.05125149960319201\n",
      "Valid loss 0.0686856811600072\n",
      "#################\n",
      "###Epoch: 71\n",
      "Train loss 0.050422641828104305\n",
      "Valid loss 0.06931521317788533\n",
      "#################\n",
      "###Epoch: 72\n",
      "Train loss 0.050195600285574245\n",
      "Valid loss 0.07032338795917374\n",
      "#################\n",
      "###Epoch: 73\n",
      "Train loss 0.049863143237652605\n",
      "Valid loss 0.06973117323858398\n",
      "#################\n",
      "###Epoch: 74\n",
      "Train loss 0.04950869442136199\n",
      "Valid loss 0.06799618154764175\n",
      "#################\n",
      "###Epoch: 75\n",
      "Train loss 0.048925524232564147\n",
      "Valid loss 0.0710226728447846\n",
      "#################\n",
      "###Epoch: 76\n",
      "Train loss 0.04923691517776913\n",
      "Valid loss 0.06820598830069814\n",
      "#################\n",
      "###Epoch: 77\n",
      "Train loss 0.04969342922170957\n",
      "Valid loss 0.06931467726826668\n",
      "#################\n",
      "###Epoch: 78\n",
      "Train loss 0.04838261298007435\n",
      "Valid loss 0.0689100708280291\n",
      "#################\n",
      "###Epoch: 79\n",
      "Train loss 0.04845075971550412\n",
      "Valid loss 0.06775079986878804\n",
      "#################\n",
      "###Epoch: 80\n",
      "Train loss 0.047886271167684485\n",
      "Valid loss 0.06904356075184685\n",
      "#################\n",
      "###Epoch: 81\n",
      "Train loss 0.04761682988868819\n",
      "Valid loss 0.06958921626210213\n",
      "#################\n",
      "###Epoch: 82\n",
      "Train loss 0.04706631918196325\n",
      "Valid loss 0.06779233206595693\n",
      "#################\n",
      "###Epoch: 83\n",
      "Train loss 0.04656929012250017\n",
      "Valid loss 0.06893865231956754\n",
      "#################\n",
      "###Epoch: 84\n",
      "Train loss 0.04689520086955141\n",
      "Valid loss 0.06755192471402031\n",
      "#################\n",
      "###Epoch: 85\n",
      "Train loss 0.04624676538838281\n",
      "Valid loss 0.06822185058678899\n",
      "#################\n",
      "###Epoch: 86\n",
      "Train loss 0.045946679871391366\n",
      "Valid loss 0.06828755778925759\n",
      "#################\n",
      "###Epoch: 87\n",
      "Train loss 0.045881865477120434\n",
      "Valid loss 0.06817320308515004\n",
      "#################\n",
      "###Epoch: 88\n",
      "Train loss 0.04577789552233837\n",
      "Valid loss 0.06828116731984275\n",
      "#################\n",
      "###Epoch: 89\n",
      "Train loss 0.047044704357783\n",
      "Valid loss 0.06894175495420184\n",
      "#################\n",
      "###Epoch: 90\n",
      "Train loss 0.04519908806240117\n",
      "Valid loss 0.06752833404711314\n",
      "#################\n",
      "###Epoch: 91\n",
      "Train loss 0.04482551470950798\n",
      "Valid loss 0.06731717767460006\n",
      "#################\n",
      "###Epoch: 92\n",
      "Train loss 0.04459134958408497\n",
      "Valid loss 0.06845164511884962\n",
      "#################\n",
      "###Epoch: 93\n",
      "Train loss 0.044998227032246416\n",
      "Valid loss 0.06795817986130714\n",
      "#################\n",
      "###Epoch: 94\n",
      "Train loss 0.04432329728647515\n",
      "Valid loss 0.06857506824391228\n",
      "#################\n",
      "###Epoch: 95\n",
      "Train loss 0.04360919569929441\n",
      "Valid loss 0.06794527918100357\n",
      "#################\n",
      "###Epoch: 96\n",
      "Train loss 0.0437704969059538\n",
      "Valid loss 0.06683152594736644\n",
      "#################\n",
      "###Epoch: 97\n",
      "Train loss 0.04401285118526883\n",
      "Valid loss 0.06707459901060377\n",
      "#################\n",
      "###Epoch: 98\n",
      "Train loss 0.04372928636493506\n",
      "Valid loss 0.06780114344188146\n",
      "#################\n",
      "###Epoch: 99\n",
      "Train loss 0.04342803207260591\n",
      "Valid loss 0.06653913908771106\n",
      "#################\n",
      "###Epoch: 0\n",
      "Train loss 0.2507557322581609\n",
      "Valid loss 0.19671997215066636\n",
      "#################\n",
      "###Epoch: 1\n",
      "Train loss 0.17680856364744682\n",
      "Valid loss 0.1619944657598223\n",
      "#################\n",
      "###Epoch: 2\n",
      "Train loss 0.15539201349020004\n",
      "Valid loss 0.14518135786056519\n",
      "#################\n",
      "###Epoch: 3\n",
      "Train loss 0.13983586402954878\n",
      "Valid loss 0.13002633090530122\n",
      "#################\n",
      "###Epoch: 4\n",
      "Train loss 0.12986337945417123\n",
      "Valid loss 0.12349011536155428\n",
      "#################\n",
      "###Epoch: 5\n",
      "Train loss 0.12360168248414993\n",
      "Valid loss 0.11875455507210322\n",
      "#################\n",
      "###Epoch: 6\n",
      "Train loss 0.12044475751894491\n",
      "Valid loss 0.11831047385931015\n",
      "#################\n",
      "###Epoch: 7\n",
      "Train loss 0.1168583435592828\n",
      "Valid loss 0.1143399008682796\n",
      "#################\n",
      "###Epoch: 8\n",
      "Train loss 0.11438041511509153\n",
      "Valid loss 0.11225664615631104\n",
      "#################\n",
      "###Epoch: 9\n",
      "Train loss 0.11218707180685467\n",
      "Valid loss 0.10970449447631836\n",
      "#################\n",
      "###Epoch: 10\n",
      "Train loss 0.1085266250151175\n",
      "Valid loss 0.10260539182594844\n",
      "#################\n",
      "###Epoch: 11\n",
      "Train loss 0.10573553349132891\n",
      "Valid loss 0.10481125967843193\n",
      "#################\n",
      "###Epoch: 12\n",
      "Train loss 0.10349049750301573\n",
      "Valid loss 0.10129323175975255\n",
      "#################\n",
      "###Epoch: 13\n",
      "Train loss 0.10075958404276106\n",
      "Valid loss 0.09781620864357267\n",
      "#################\n",
      "###Epoch: 14\n",
      "Train loss 0.09752956087942477\n",
      "Valid loss 0.09789924110685076\n",
      "#################\n",
      "###Epoch: 15\n",
      "Train loss 0.09548157701889674\n",
      "Valid loss 0.09283077823264259\n",
      "#################\n",
      "###Epoch: 16\n",
      "Train loss 0.093561714997998\n",
      "Valid loss 0.09240256143467766\n",
      "#################\n",
      "###Epoch: 17\n",
      "Train loss 0.09196741299496757\n",
      "Valid loss 0.09030289202928543\n",
      "#################\n",
      "###Epoch: 18\n",
      "Train loss 0.08928493493133122\n",
      "Valid loss 0.08787534598793302\n",
      "#################\n",
      "###Epoch: 19\n",
      "Train loss 0.08738072437268717\n",
      "Valid loss 0.0888555816241673\n",
      "#################\n",
      "###Epoch: 20\n",
      "Train loss 0.08741264128022724\n",
      "Valid loss 0.08480371002640043\n",
      "#################\n",
      "###Epoch: 21\n",
      "Train loss 0.08329498243552667\n",
      "Valid loss 0.0817496052810124\n",
      "#################\n",
      "###Epoch: 22\n",
      "Train loss 0.08254416562892773\n",
      "Valid loss 0.07951586587088448\n",
      "#################\n",
      "###Epoch: 23\n",
      "Train loss 0.08081237281914111\n",
      "Valid loss 0.07850554904767446\n",
      "#################\n",
      "###Epoch: 24\n",
      "Train loss 0.07881730298201244\n",
      "Valid loss 0.07853151644979205\n",
      "#################\n",
      "###Epoch: 25\n",
      "Train loss 0.07765780334119443\n",
      "Valid loss 0.07824351957866124\n",
      "#################\n",
      "###Epoch: 26\n",
      "Train loss 0.07596727405433301\n",
      "Valid loss 0.07607816691909518\n",
      "#################\n",
      "###Epoch: 27\n",
      "Train loss 0.0741246384051111\n",
      "Valid loss 0.07449323471103396\n",
      "#################\n",
      "###Epoch: 28\n",
      "Train loss 0.07447745444046126\n",
      "Valid loss 0.0742050079362733\n",
      "#################\n",
      "###Epoch: 29\n",
      "Train loss 0.072942397798653\n",
      "Valid loss 0.07201347819396428\n",
      "#################\n",
      "###Epoch: 30\n",
      "Train loss 0.07185197318041767\n",
      "Valid loss 0.07520698543105807\n",
      "#################\n",
      "###Epoch: 31\n",
      "Train loss 0.06995186140691792\n",
      "Valid loss 0.07136967352458409\n",
      "#################\n",
      "###Epoch: 32\n",
      "Train loss 0.06997346698685929\n",
      "Valid loss 0.07289547366755349\n",
      "#################\n",
      "###Epoch: 33\n",
      "Train loss 0.07001378083670581\n",
      "Valid loss 0.07069903665355273\n",
      "#################\n",
      "###Epoch: 34\n",
      "Train loss 0.06906637866739873\n",
      "Valid loss 0.0723164017711367\n",
      "#################\n",
      "###Epoch: 35\n",
      "Train loss 0.06790595129132271\n",
      "Valid loss 0.06936254352331161\n",
      "#################\n",
      "###Epoch: 36\n",
      "Train loss 0.06644979022719243\n",
      "Valid loss 0.06891409520592008\n",
      "#################\n",
      "###Epoch: 37\n",
      "Train loss 0.06723171983052183\n",
      "Valid loss 0.07040526930774961\n",
      "#################\n",
      "###Epoch: 38\n",
      "Train loss 0.06542098039278278\n",
      "Valid loss 0.07320927083492279\n",
      "#################\n",
      "###Epoch: 39\n",
      "Train loss 0.06513716901342075\n",
      "Valid loss 0.06911144139511245\n",
      "#################\n",
      "###Epoch: 40\n",
      "Train loss 0.06475531723764208\n",
      "Valid loss 0.06936518315758024\n",
      "#################\n",
      "###Epoch: 41\n",
      "Train loss 0.0642035253070019\n",
      "Valid loss 0.06986883176224572\n",
      "#################\n",
      "###Epoch: 42\n",
      "Train loss 0.06336878571245405\n",
      "Valid loss 0.06744090733783585\n",
      "#################\n",
      "###Epoch: 43\n",
      "Train loss 0.0618176130508935\n",
      "Valid loss 0.06896004027553967\n",
      "#################\n",
      "###Epoch: 44\n",
      "Train loss 0.06161277961951715\n",
      "Valid loss 0.06729066371917725\n",
      "#################\n",
      "###Epoch: 45\n",
      "Train loss 0.060779675427410335\n",
      "Valid loss 0.06720710439341408\n",
      "#################\n",
      "###Epoch: 46\n",
      "Train loss 0.06059221109306371\n",
      "Valid loss 0.0670414689396109\n",
      "#################\n",
      "###Epoch: 47\n",
      "Train loss 0.060290540672010846\n",
      "Valid loss 0.06755191566688674\n",
      "#################\n",
      "###Epoch: 48\n",
      "Train loss 0.05904820188879967\n",
      "Valid loss 0.06578456610441208\n",
      "#################\n",
      "###Epoch: 49\n",
      "Train loss 0.05872562982969814\n",
      "Valid loss 0.0669590128319604\n",
      "#################\n",
      "###Epoch: 50\n",
      "Train loss 0.05857991827306924\n",
      "Valid loss 0.06609107234648295\n",
      "#################\n",
      "###Epoch: 51\n",
      "Train loss 0.05815142972601785\n",
      "Valid loss 0.06522170613918986\n",
      "#################\n",
      "###Epoch: 52\n",
      "Train loss 0.057052510066164866\n",
      "Valid loss 0.06676391937902995\n",
      "#################\n",
      "###Epoch: 53\n",
      "Train loss 0.057501743375151244\n",
      "Valid loss 0.06943640698279653\n",
      "#################\n",
      "###Epoch: 54\n",
      "Train loss 0.056525074083496024\n",
      "Valid loss 0.06626339948603086\n",
      "#################\n",
      "###Epoch: 55\n",
      "Train loss 0.05663828827716686\n",
      "Valid loss 0.06755496721182551\n",
      "#################\n",
      "###Epoch: 56\n",
      "Train loss 0.05562596133461705\n",
      "Valid loss 0.06513176166585513\n",
      "#################\n",
      "###Epoch: 57\n",
      "Train loss 0.05569742537207074\n",
      "Valid loss 0.06674431370837348\n",
      "#################\n",
      "###Epoch: 58\n",
      "Train loss 0.056708610996052074\n",
      "Valid loss 0.06705687354717936\n",
      "#################\n",
      "###Epoch: 59\n",
      "Train loss 0.0550435782858619\n",
      "Valid loss 0.06629723204033715\n",
      "#################\n",
      "###Epoch: 60\n",
      "Train loss 0.05404800659528485\n",
      "Valid loss 0.06540820854050773\n",
      "#################\n",
      "###Epoch: 61\n",
      "Train loss 0.054259078922095125\n",
      "Valid loss 0.0664609441799777\n",
      "#################\n",
      "###Epoch: 62\n",
      "Train loss 0.053398218005895615\n",
      "Valid loss 0.06658257703695979\n",
      "#################\n",
      "###Epoch: 63\n",
      "Train loss 0.053035189708073936\n",
      "Valid loss 0.06518856808543205\n",
      "#################\n",
      "###Epoch: 64\n",
      "Train loss 0.052618546342408215\n",
      "Valid loss 0.06530717653887612\n",
      "#################\n",
      "###Epoch: 65\n",
      "Train loss 0.05230829036898083\n",
      "Valid loss 0.06465168563382966\n",
      "#################\n",
      "###Epoch: 66\n",
      "Train loss 0.05218314407048402\n",
      "Valid loss 0.06563237256237439\n",
      "#################\n",
      "###Epoch: 67\n",
      "Train loss 0.051882891881245154\n",
      "Valid loss 0.0642682606620448\n",
      "#################\n",
      "###Epoch: 68\n",
      "Train loss 0.05120110663550871\n",
      "Valid loss 0.06503281795552798\n",
      "#################\n",
      "###Epoch: 69\n",
      "Train loss 0.05173490458616504\n",
      "Valid loss 0.06359672333512988\n",
      "#################\n",
      "###Epoch: 70\n",
      "Train loss 0.050746203020766927\n",
      "Valid loss 0.06588903548462051\n",
      "#################\n",
      "###Epoch: 71\n",
      "Train loss 0.05050204542500002\n",
      "Valid loss 0.06454417056271008\n",
      "#################\n",
      "###Epoch: 72\n",
      "Train loss 0.050198936352023375\n",
      "Valid loss 0.0644362520958696\n",
      "#################\n",
      "###Epoch: 73\n",
      "Train loss 0.049654813276396856\n",
      "Valid loss 0.0646934402840478\n",
      "#################\n",
      "###Epoch: 74\n",
      "Train loss 0.04951028084313428\n",
      "Valid loss 0.0636826040489333\n",
      "#################\n",
      "###Epoch: 75\n",
      "Train loss 0.049338350969332236\n",
      "Valid loss 0.06437991133758\n",
      "#################\n",
      "###Epoch: 76\n",
      "Train loss 0.04920333858441423\n",
      "Valid loss 0.06411459190504891\n",
      "#################\n",
      "###Epoch: 77\n",
      "Train loss 0.048705854211692455\n",
      "Valid loss 0.0648547171482018\n",
      "#################\n",
      "###Epoch: 78\n",
      "Train loss 0.04800136911648291\n",
      "Valid loss 0.06423684369240489\n",
      "#################\n",
      "###Epoch: 79\n",
      "Train loss 0.04760493586460749\n",
      "Valid loss 0.06390441209077835\n",
      "#################\n",
      "###Epoch: 80\n",
      "Train loss 0.04779868697126707\n",
      "Valid loss 0.06406192162207194\n",
      "#################\n",
      "###Epoch: 81\n",
      "Train loss 0.04726978515585264\n",
      "Valid loss 0.06434966836656843\n",
      "#################\n",
      "###Epoch: 82\n",
      "Train loss 0.04700502035794435\n",
      "Valid loss 0.06449419153588158\n",
      "#################\n",
      "###Epoch: 83\n",
      "Train loss 0.046452007083981124\n",
      "Valid loss 0.06493162150893893\n",
      "#################\n",
      "###Epoch: 84\n",
      "Train loss 0.046568036907249026\n",
      "Valid loss 0.06548558760966573\n",
      "#################\n",
      "###Epoch: 85\n",
      "Train loss 0.04637569244261141\n",
      "Valid loss 0.06426749059132167\n",
      "#################\n",
      "###Epoch: 86\n",
      "Train loss 0.045916706737544805\n",
      "Valid loss 0.06430678335683686\n",
      "#################\n",
      "###Epoch: 87\n",
      "Train loss 0.04546508193016052\n",
      "Valid loss 0.06320500852806228\n",
      "#################\n",
      "###Epoch: 88\n",
      "Train loss 0.04558624092627455\n",
      "Valid loss 0.06676920503377914\n",
      "#################\n",
      "###Epoch: 89\n",
      "Train loss 0.04586484766116849\n",
      "Valid loss 0.06561460665294103\n",
      "#################\n",
      "###Epoch: 90\n",
      "Train loss 0.045627389655069066\n",
      "Valid loss 0.06517063240919795\n",
      "#################\n",
      "###Epoch: 91\n",
      "Train loss 0.04493015245706947\n",
      "Valid loss 0.06380327524883407\n",
      "#################\n",
      "###Epoch: 92\n",
      "Train loss 0.04497440793999919\n",
      "Valid loss 0.06378234922885895\n",
      "#################\n",
      "###Epoch: 93\n",
      "Train loss 0.044438073618544474\n",
      "Valid loss 0.06425889155694417\n",
      "#################\n",
      "###Epoch: 94\n",
      "Train loss 0.04442522495433136\n",
      "Valid loss 0.06385929988963264\n",
      "#################\n",
      "###Epoch: 95\n",
      "Train loss 0.044122469507985644\n",
      "Valid loss 0.06317200032728058\n",
      "#################\n",
      "###Epoch: 96\n",
      "Train loss 0.04426594395880346\n",
      "Valid loss 0.06350901137505259\n",
      "#################\n",
      "###Epoch: 97\n",
      "Train loss 0.04403102549689787\n",
      "Valid loss 0.06400772343788828\n",
      "#################\n",
      "###Epoch: 98\n",
      "Train loss 0.04367244271216569\n",
      "Valid loss 0.06543615673269544\n",
      "#################\n",
      "###Epoch: 99\n",
      "Train loss 0.043384466458249976\n",
      "Valid loss 0.06334573775529861\n",
      "#################\n",
      "###Epoch: 0\n",
      "Train loss 0.2571484358222396\n",
      "Valid loss 0.19944974780082703\n",
      "#################\n",
      "###Epoch: 1\n",
      "Train loss 0.1861197661470484\n",
      "Valid loss 0.16525587226663316\n",
      "#################\n",
      "###Epoch: 2\n",
      "Train loss 0.16396326378539758\n",
      "Valid loss 0.14482583957059042\n",
      "#################\n",
      "###Epoch: 3\n",
      "Train loss 0.14694914442521553\n",
      "Valid loss 0.12964410654136113\n",
      "#################\n",
      "###Epoch: 4\n",
      "Train loss 0.13299651168010854\n",
      "Valid loss 0.12305334423269544\n",
      "#################\n",
      "###Epoch: 5\n",
      "Train loss 0.12585920868096528\n",
      "Valid loss 0.115233556500503\n",
      "#################\n",
      "###Epoch: 6\n",
      "Train loss 0.12407217340336905\n",
      "Valid loss 0.11608606364045825\n",
      "#################\n",
      "###Epoch: 7\n",
      "Train loss 0.12028455706658187\n",
      "Valid loss 0.11245934345892497\n",
      "#################\n",
      "###Epoch: 8\n",
      "Train loss 0.11700798019214913\n",
      "Valid loss 0.11019111637558256\n",
      "#################\n",
      "###Epoch: 9\n",
      "Train loss 0.11487391259935167\n",
      "Valid loss 0.10658570591892515\n",
      "#################\n",
      "###Epoch: 10\n",
      "Train loss 0.11176691756204322\n",
      "Valid loss 0.10329511016607285\n",
      "#################\n",
      "###Epoch: 11\n",
      "Train loss 0.1102360823640117\n",
      "Valid loss 0.10012286795037133\n",
      "#################\n",
      "###Epoch: 12\n",
      "Train loss 0.1049656316086098\n",
      "Valid loss 0.09737327269145421\n",
      "#################\n",
      "###Epoch: 13\n",
      "Train loss 0.10161446779966354\n",
      "Valid loss 0.09407881860222135\n",
      "#################\n",
      "###Epoch: 14\n",
      "Train loss 0.09936169562516389\n",
      "Valid loss 0.09154767543077469\n",
      "#################\n",
      "###Epoch: 15\n",
      "Train loss 0.097761618870276\n",
      "Valid loss 0.08982555993965693\n",
      "#################\n",
      "###Epoch: 16\n",
      "Train loss 0.09520128976415705\n",
      "Valid loss 0.08862741823707308\n",
      "#################\n",
      "###Epoch: 17\n",
      "Train loss 0.09359295473054603\n",
      "Valid loss 0.08681538594620568\n",
      "#################\n",
      "###Epoch: 18\n",
      "Train loss 0.0908286014088878\n",
      "Valid loss 0.08645362619842802\n",
      "#################\n",
      "###Epoch: 19\n",
      "Train loss 0.08941239808444623\n",
      "Valid loss 0.08200436511210032\n",
      "#################\n",
      "###Epoch: 20\n",
      "Train loss 0.08773393432299297\n",
      "Valid loss 0.08015909258808408\n",
      "#################\n",
      "###Epoch: 21\n",
      "Train loss 0.08474206372543618\n",
      "Valid loss 0.0789568621133055\n",
      "#################\n",
      "###Epoch: 22\n",
      "Train loss 0.08434600907343405\n",
      "Valid loss 0.07973274003182139\n",
      "#################\n",
      "###Epoch: 23\n",
      "Train loss 0.08211587866147359\n",
      "Valid loss 0.07552498579025269\n",
      "#################\n",
      "###Epoch: 24\n",
      "Train loss 0.08060419821628818\n",
      "Valid loss 0.07497454221759524\n",
      "#################\n",
      "###Epoch: 25\n",
      "Train loss 0.07855318127958863\n",
      "Valid loss 0.07443445761288915\n",
      "#################\n",
      "###Epoch: 26\n",
      "Train loss 0.07899917524169993\n",
      "Valid loss 0.07446436743651118\n",
      "#################\n",
      "###Epoch: 27\n",
      "Train loss 0.07761592666308086\n",
      "Valid loss 0.07373986446431705\n",
      "#################\n",
      "###Epoch: 28\n",
      "Train loss 0.07556733130304902\n",
      "Valid loss 0.07085696075643812\n",
      "#################\n",
      "###Epoch: 29\n",
      "Train loss 0.07429309823998699\n",
      "Valid loss 0.07137625185506684\n",
      "#################\n",
      "###Epoch: 30\n",
      "Train loss 0.07403805079283537\n",
      "Valid loss 0.06962506206972259\n",
      "#################\n",
      "###Epoch: 31\n",
      "Train loss 0.07249121947420968\n",
      "Valid loss 0.06982999028904098\n",
      "#################\n",
      "###Epoch: 32\n",
      "Train loss 0.07135503007857888\n",
      "Valid loss 0.06754319156919207\n",
      "#################\n",
      "###Epoch: 33\n",
      "Train loss 0.07008242952050986\n",
      "Valid loss 0.06969373034579414\n",
      "#################\n",
      "###Epoch: 34\n",
      "Train loss 0.06938355540235837\n",
      "Valid loss 0.06801692768931389\n",
      "#################\n",
      "###Epoch: 35\n",
      "Train loss 0.06887221488135832\n",
      "Valid loss 0.06863342651299068\n",
      "#################\n",
      "###Epoch: 36\n",
      "Train loss 0.06887566091285811\n",
      "Valid loss 0.07050432158367974\n",
      "#################\n",
      "###Epoch: 37\n",
      "Train loss 0.06886994976688314\n",
      "Valid loss 0.06548706335680825\n",
      "#################\n",
      "###Epoch: 38\n",
      "Train loss 0.0671669643510271\n",
      "Valid loss 0.06574852711388043\n",
      "#################\n",
      "###Epoch: 39\n",
      "Train loss 0.06728630126626403\n",
      "Valid loss 0.06492860242724419\n",
      "#################\n",
      "###Epoch: 40\n",
      "Train loss 0.06599015977095675\n",
      "Valid loss 0.06424986092107636\n",
      "#################\n",
      "###Epoch: 41\n",
      "Train loss 0.06517112779396551\n",
      "Valid loss 0.06710759443896157\n",
      "#################\n",
      "###Epoch: 42\n",
      "Train loss 0.06476813251221622\n",
      "Valid loss 0.06453414846743856\n",
      "#################\n",
      "###Epoch: 43\n",
      "Train loss 0.06373145776214423\n",
      "Valid loss 0.06430614367127419\n",
      "#################\n",
      "###Epoch: 44\n",
      "Train loss 0.06464235208652637\n",
      "Valid loss 0.06356854417494365\n",
      "#################\n",
      "###Epoch: 45\n",
      "Train loss 0.0634567086343412\n",
      "Valid loss 0.06557179721338409\n",
      "#################\n",
      "###Epoch: 46\n",
      "Train loss 0.06313172258712628\n",
      "Valid loss 0.06335133527006422\n",
      "#################\n",
      "###Epoch: 47\n",
      "Train loss 0.0621255598962307\n",
      "Valid loss 0.0623736817921911\n",
      "#################\n",
      "###Epoch: 48\n",
      "Train loss 0.06270007871919209\n",
      "Valid loss 0.06224721137966428\n",
      "#################\n",
      "###Epoch: 49\n",
      "Train loss 0.061766242815388575\n",
      "Valid loss 0.06255900167993136\n",
      "#################\n",
      "###Epoch: 50\n",
      "Train loss 0.060083060490864294\n",
      "Valid loss 0.06292176193424634\n",
      "#################\n",
      "###Epoch: 51\n",
      "Train loss 0.06025299761030409\n",
      "Valid loss 0.0620086986039366\n",
      "#################\n",
      "###Epoch: 52\n",
      "Train loss 0.06091828865033609\n",
      "Valid loss 0.06181705317326954\n",
      "#################\n",
      "###Epoch: 53\n",
      "Train loss 0.05988343984440521\n",
      "Valid loss 0.061264289276940484\n",
      "#################\n",
      "###Epoch: 54\n",
      "Train loss 0.05940532284202399\n",
      "Valid loss 0.06318401172757149\n",
      "#################\n",
      "###Epoch: 55\n",
      "Train loss 0.05894364885709904\n",
      "Valid loss 0.06062029195683343\n",
      "#################\n",
      "###Epoch: 56\n",
      "Train loss 0.05846504315182015\n",
      "Valid loss 0.060353481875998635\n",
      "#################\n",
      "###Epoch: 57\n",
      "Train loss 0.0574569473112071\n",
      "Valid loss 0.06021670890705926\n",
      "#################\n",
      "###Epoch: 58\n",
      "Train loss 0.05708723087553625\n",
      "Valid loss 0.060650430619716644\n",
      "#################\n",
      "###Epoch: 59\n",
      "Train loss 0.05612902191502077\n",
      "Valid loss 0.05990015821797507\n",
      "#################\n",
      "###Epoch: 60\n",
      "Train loss 0.05615693579117457\n",
      "Valid loss 0.06032711746437209\n",
      "#################\n",
      "###Epoch: 61\n",
      "Train loss 0.05557949634061919\n",
      "Valid loss 0.06040634374533381\n",
      "#################\n",
      "###Epoch: 62\n",
      "Train loss 0.055113070402984264\n",
      "Valid loss 0.05989874580076763\n",
      "#################\n",
      "###Epoch: 63\n",
      "Train loss 0.05542527111592116\n",
      "Valid loss 0.06178997776338032\n",
      "#################\n",
      "###Epoch: 64\n",
      "Train loss 0.05577107643087705\n",
      "Valid loss 0.05961894829358373\n",
      "#################\n",
      "###Epoch: 65\n",
      "Train loss 0.05444808304309845\n",
      "Valid loss 0.060351958232266564\n",
      "#################\n",
      "###Epoch: 66\n",
      "Train loss 0.054090568174918495\n",
      "Valid loss 0.05903865450194904\n",
      "#################\n",
      "###Epoch: 67\n",
      "Train loss 0.0536870437639731\n",
      "Valid loss 0.058936472449983866\n",
      "#################\n",
      "###Epoch: 68\n",
      "Train loss 0.05344101165731748\n",
      "Valid loss 0.059403958065169196\n",
      "#################\n",
      "###Epoch: 69\n",
      "Train loss 0.05453447307701464\n",
      "Valid loss 0.06042322942188808\n",
      "#################\n",
      "###Epoch: 70\n",
      "Train loss 0.053911534310491\n",
      "Valid loss 0.05856481886335781\n",
      "#################\n",
      "###Epoch: 71\n",
      "Train loss 0.052400520692269005\n",
      "Valid loss 0.05937950046999114\n",
      "#################\n",
      "###Epoch: 72\n",
      "Train loss 0.052320825418940294\n",
      "Valid loss 0.05893055509243693\n",
      "#################\n",
      "###Epoch: 73\n",
      "Train loss 0.05236961375232096\n",
      "Valid loss 0.05914340753640447\n",
      "#################\n",
      "###Epoch: 74\n",
      "Train loss 0.05225712902568005\n",
      "Valid loss 0.059981923018183024\n",
      "#################\n",
      "###Epoch: 75\n",
      "Train loss 0.05233914336120641\n",
      "Valid loss 0.05815568193793297\n",
      "#################\n",
      "###Epoch: 76\n",
      "Train loss 0.051287349589444975\n",
      "Valid loss 0.058259879904133935\n",
      "#################\n",
      "###Epoch: 77\n",
      "Train loss 0.05070049205312022\n",
      "Valid loss 0.05830483138561249\n",
      "#################\n",
      "###Epoch: 78\n",
      "Train loss 0.05068277774585618\n",
      "Valid loss 0.05797533690929413\n",
      "#################\n",
      "###Epoch: 79\n",
      "Train loss 0.05008992552757263\n",
      "Valid loss 0.058380589953490665\n",
      "#################\n",
      "###Epoch: 80\n",
      "Train loss 0.05079765521265842\n",
      "Valid loss 0.05920390837958881\n",
      "#################\n",
      "###Epoch: 81\n",
      "Train loss 0.049762140683553835\n",
      "Valid loss 0.05848925454275949\n",
      "#################\n",
      "###Epoch: 82\n",
      "Train loss 0.04924648324096644\n",
      "Valid loss 0.05812584608793259\n",
      "#################\n",
      "###Epoch: 83\n",
      "Train loss 0.04911825336791851\n",
      "Valid loss 0.058245857911450524\n",
      "#################\n",
      "###Epoch: 84\n",
      "Train loss 0.04953965461916394\n",
      "Valid loss 0.0582694374024868\n",
      "#################\n",
      "###Epoch: 85\n",
      "Train loss 0.048564198116461434\n",
      "Valid loss 0.05739007091947964\n",
      "#################\n",
      "###Epoch: 86\n",
      "Train loss 0.04841357193611286\n",
      "Valid loss 0.057890044791357856\n",
      "#################\n",
      "###Epoch: 87\n",
      "Train loss 0.048463534425806115\n",
      "Valid loss 0.057593491460595815\n",
      "#################\n",
      "###Epoch: 88\n",
      "Train loss 0.04792631845231409\n",
      "Valid loss 0.05806349377546992\n",
      "#################\n",
      "###Epoch: 89\n",
      "Train loss 0.047436562125329616\n",
      "Valid loss 0.05747157494936671\n",
      "#################\n",
      "###Epoch: 90\n",
      "Train loss 0.04718093822399775\n",
      "Valid loss 0.0583365261554718\n",
      "#################\n",
      "###Epoch: 91\n",
      "Train loss 0.04685062114839201\n",
      "Valid loss 0.05757643921034677\n",
      "#################\n",
      "###Epoch: 92\n",
      "Train loss 0.046879790171428966\n",
      "Valid loss 0.05811753070780209\n",
      "#################\n",
      "###Epoch: 93\n",
      "Train loss 0.04657302010390493\n",
      "Valid loss 0.05775075512272971\n",
      "#################\n",
      "###Epoch: 94\n",
      "Train loss 0.04670228367602384\n",
      "Valid loss 0.057261775114706585\n",
      "#################\n",
      "###Epoch: 95\n",
      "Train loss 0.04687874778001397\n",
      "Valid loss 0.057439700833388736\n",
      "#################\n",
      "###Epoch: 96\n",
      "Train loss 0.046011239014289995\n",
      "Valid loss 0.058682672679424286\n",
      "#################\n",
      "###Epoch: 97\n",
      "Train loss 0.04659240706651299\n",
      "Valid loss 0.05724530720285007\n",
      "#################\n",
      "###Epoch: 98\n",
      "Train loss 0.04595641805617898\n",
      "Valid loss 0.05673526067818914\n",
      "#################\n",
      "###Epoch: 99\n",
      "Train loss 0.046159113860792585\n",
      "Valid loss 0.05836143717169762\n",
      "#################\n",
      "###Epoch: 0\n",
      "Train loss 0.25415702495310044\n",
      "Valid loss 0.19391813661370957\n",
      "#################\n",
      "###Epoch: 1\n",
      "Train loss 0.18401015135977003\n",
      "Valid loss 0.1642426699399948\n",
      "#################\n",
      "###Epoch: 2\n",
      "Train loss 0.1628556963470247\n",
      "Valid loss 0.14932614139148168\n",
      "#################\n",
      "###Epoch: 3\n",
      "Train loss 0.1469470833738645\n",
      "Valid loss 0.13515536061355046\n",
      "#################\n",
      "###Epoch: 4\n",
      "Train loss 0.13567327487247963\n",
      "Valid loss 0.1292723992041179\n",
      "#################\n",
      "###Epoch: 5\n",
      "Train loss 0.12751068505975935\n",
      "Valid loss 0.12196715176105499\n",
      "#################\n",
      "###Epoch: 6\n",
      "Train loss 0.12143434998061922\n",
      "Valid loss 0.11828445430312838\n",
      "#################\n",
      "###Epoch: 7\n",
      "Train loss 0.11855937595720645\n",
      "Valid loss 0.11593599936791829\n",
      "#################\n",
      "###Epoch: 8\n",
      "Train loss 0.11470680766635472\n",
      "Valid loss 0.11124578650508608\n",
      "#################\n",
      "###Epoch: 9\n",
      "Train loss 0.11169814953097591\n",
      "Valid loss 0.10745889587061745\n",
      "#################\n",
      "###Epoch: 10\n",
      "Train loss 0.10826626585589515\n",
      "Valid loss 0.1136997863650322\n",
      "#################\n",
      "###Epoch: 11\n",
      "Train loss 0.10553176756258364\n",
      "Valid loss 0.10396440327167511\n",
      "#################\n",
      "###Epoch: 12\n",
      "Train loss 0.10220876060150287\n",
      "Valid loss 0.10095683911017009\n",
      "#################\n",
      "###Epoch: 13\n",
      "Train loss 0.10034525201276497\n",
      "Valid loss 0.09946357139519282\n",
      "#################\n",
      "###Epoch: 14\n",
      "Train loss 0.09753477573394775\n",
      "Valid loss 0.09503194157566343\n",
      "#################\n",
      "###Epoch: 15\n",
      "Train loss 0.09537066105339262\n",
      "Valid loss 0.0950134779725756\n",
      "#################\n",
      "###Epoch: 16\n",
      "Train loss 0.09482299702035056\n",
      "Valid loss 0.09303703265530723\n",
      "#################\n",
      "###Epoch: 17\n",
      "Train loss 0.09226829282663486\n",
      "Valid loss 0.09042995848825999\n",
      "#################\n",
      "###Epoch: 18\n",
      "Train loss 0.09155145922192821\n",
      "Valid loss 0.08926758170127869\n",
      "#################\n",
      "###Epoch: 19\n",
      "Train loss 0.0877111377539458\n",
      "Valid loss 0.08720670427594866\n",
      "#################\n",
      "###Epoch: 20\n",
      "Train loss 0.08624031505099049\n",
      "Valid loss 0.08843860243047987\n",
      "#################\n",
      "###Epoch: 21\n",
      "Train loss 0.0849616770391111\n",
      "Valid loss 0.09065657215459007\n",
      "#################\n",
      "###Epoch: 22\n",
      "Train loss 0.08683116734027863\n",
      "Valid loss 0.08753051928111485\n",
      "#################\n",
      "###Epoch: 23\n",
      "Train loss 0.08342284847188879\n",
      "Valid loss 0.08253849084888186\n",
      "#################\n",
      "###Epoch: 24\n",
      "Train loss 0.08264846520291434\n",
      "Valid loss 0.08193552174738475\n",
      "#################\n",
      "###Epoch: 25\n",
      "Train loss 0.08098289011805146\n",
      "Valid loss 0.07938068466527122\n",
      "#################\n",
      "###Epoch: 26\n",
      "Train loss 0.0779309457650891\n",
      "Valid loss 0.07793040573596954\n",
      "#################\n",
      "###Epoch: 27\n",
      "Train loss 0.07665290766292149\n",
      "Valid loss 0.07815647018807274\n",
      "#################\n",
      "###Epoch: 28\n",
      "Train loss 0.07589966941762853\n",
      "Valid loss 0.07648912923676628\n",
      "#################\n",
      "###Epoch: 29\n",
      "Train loss 0.07562124991306553\n",
      "Valid loss 0.07619567002568926\n",
      "#################\n",
      "###Epoch: 30\n",
      "Train loss 0.07307163212034437\n",
      "Valid loss 0.07692333097968783\n",
      "#################\n",
      "###Epoch: 31\n",
      "Train loss 0.07281963196065691\n",
      "Valid loss 0.07435775441782815\n",
      "#################\n",
      "###Epoch: 32\n",
      "Train loss 0.07153897208196146\n",
      "Valid loss 0.07528428414038249\n",
      "#################\n",
      "###Epoch: 33\n",
      "Train loss 0.07038251020842129\n",
      "Valid loss 0.07398501836827823\n",
      "#################\n",
      "###Epoch: 34\n",
      "Train loss 0.06956972209391771\n",
      "Valid loss 0.07300019477094923\n",
      "#################\n",
      "###Epoch: 35\n",
      "Train loss 0.06942489108553639\n",
      "Valid loss 0.07263769581913948\n",
      "#################\n",
      "###Epoch: 36\n",
      "Train loss 0.06944382797788691\n",
      "Valid loss 0.07286839666111129\n",
      "#################\n",
      "###Epoch: 37\n",
      "Train loss 0.06754905606309573\n",
      "Valid loss 0.07150055734174592\n",
      "#################\n",
      "###Epoch: 38\n",
      "Train loss 0.06864398331553848\n",
      "Valid loss 0.07083525827952794\n",
      "#################\n",
      "###Epoch: 39\n",
      "Train loss 0.06577369777692689\n",
      "Valid loss 0.07158760886107172\n",
      "#################\n",
      "###Epoch: 40\n",
      "Train loss 0.06555307063239592\n",
      "Valid loss 0.06934925754155431\n",
      "#################\n",
      "###Epoch: 41\n",
      "Train loss 0.06591569383939107\n",
      "Valid loss 0.07065405909504209\n",
      "#################\n",
      "###Epoch: 42\n",
      "Train loss 0.06505919579002592\n",
      "Valid loss 0.06944711240274566\n",
      "#################\n",
      "###Epoch: 43\n",
      "Train loss 0.06483973852462238\n",
      "Valid loss 0.06876962578722409\n",
      "#################\n",
      "###Epoch: 44\n",
      "Train loss 0.06397399968571132\n",
      "Valid loss 0.06853346952370235\n",
      "#################\n",
      "###Epoch: 45\n",
      "Train loss 0.06265610828995705\n",
      "Valid loss 0.06861968711018562\n",
      "#################\n",
      "###Epoch: 46\n",
      "Train loss 0.062173452918176296\n",
      "Valid loss 0.06890618322151047\n",
      "#################\n",
      "###Epoch: 47\n",
      "Train loss 0.06164563695589701\n",
      "Valid loss 0.06757490336894989\n",
      "#################\n",
      "###Epoch: 48\n",
      "Train loss 0.061202796934931365\n",
      "Valid loss 0.06689896594200816\n",
      "#################\n",
      "###Epoch: 49\n",
      "Train loss 0.06103446238018848\n",
      "Valid loss 0.06647998626743044\n",
      "#################\n",
      "###Epoch: 50\n",
      "Train loss 0.06027133072967882\n",
      "Valid loss 0.06691041429127965\n",
      "#################\n",
      "###Epoch: 51\n",
      "Train loss 0.0597514929594817\n",
      "Valid loss 0.06655523074524743\n",
      "#################\n",
      "###Epoch: 52\n",
      "Train loss 0.059540341435759155\n",
      "Valid loss 0.06657324305602483\n",
      "#################\n",
      "###Epoch: 53\n",
      "Train loss 0.05975434460021831\n",
      "Valid loss 0.06608710650886808\n",
      "#################\n",
      "###Epoch: 54\n",
      "Train loss 0.058453829338153206\n",
      "Valid loss 0.0659748667052814\n",
      "#################\n",
      "###Epoch: 55\n",
      "Train loss 0.05815262802773052\n",
      "Valid loss 0.06562680591429983\n",
      "#################\n",
      "###Epoch: 56\n",
      "Train loss 0.058073931408149225\n",
      "Valid loss 0.06625663435884885\n",
      "#################\n",
      "###Epoch: 57\n",
      "Train loss 0.05655479941655089\n",
      "Valid loss 0.06545719557574817\n",
      "#################\n",
      "###Epoch: 58\n",
      "Train loss 0.056762127964584914\n",
      "Valid loss 0.0654022369001593\n",
      "#################\n",
      "###Epoch: 59\n",
      "Train loss 0.05672504656292774\n",
      "Valid loss 0.06569498351642064\n",
      "#################\n",
      "###Epoch: 60\n",
      "Train loss 0.05561437775139456\n",
      "Valid loss 0.0693770886531898\n",
      "#################\n",
      "###Epoch: 61\n",
      "Train loss 0.055244811431125355\n",
      "Valid loss 0.0649529727441924\n",
      "#################\n",
      "###Epoch: 62\n",
      "Train loss 0.055307388581611494\n",
      "Valid loss 0.06668123336774963\n",
      "#################\n",
      "###Epoch: 63\n",
      "Train loss 0.054431440929571785\n",
      "Valid loss 0.0659032692866666\n",
      "#################\n",
      "###Epoch: 64\n",
      "Train loss 0.05478383407548622\n",
      "Valid loss 0.0655393877199718\n",
      "#################\n",
      "###Epoch: 65\n",
      "Train loss 0.05438225964705149\n",
      "Valid loss 0.06500458930219923\n",
      "#################\n",
      "###Epoch: 66\n",
      "Train loss 0.05354828525472571\n",
      "Valid loss 0.06506014934607915\n",
      "#################\n",
      "###Epoch: 67\n",
      "Train loss 0.05329767531818814\n",
      "Valid loss 0.06413475690143448\n",
      "#################\n",
      "###Epoch: 68\n",
      "Train loss 0.05333945858809683\n",
      "Valid loss 0.06438403789486204\n",
      "#################\n",
      "###Epoch: 69\n",
      "Train loss 0.05263503696079607\n",
      "Valid loss 0.06377952066915375\n",
      "#################\n",
      "###Epoch: 70\n",
      "Train loss 0.05250634404796141\n",
      "Valid loss 0.06478616861360413\n",
      "#################\n",
      "###Epoch: 71\n",
      "Train loss 0.05206043769915899\n",
      "Valid loss 0.06338632479310036\n",
      "#################\n",
      "###Epoch: 72\n",
      "Train loss 0.051642143064075045\n",
      "Valid loss 0.06460256448813848\n",
      "#################\n",
      "###Epoch: 73\n",
      "Train loss 0.052230689536642144\n",
      "Valid loss 0.06416994546140943\n",
      "#################\n",
      "###Epoch: 74\n",
      "Train loss 0.051834787207621115\n",
      "Valid loss 0.06454456704003471\n",
      "#################\n",
      "###Epoch: 75\n",
      "Train loss 0.05140126606932393\n",
      "Valid loss 0.06515953317284584\n",
      "#################\n",
      "###Epoch: 76\n",
      "Train loss 0.050759944788835665\n",
      "Valid loss 0.06337805837392807\n",
      "#################\n",
      "###Epoch: 77\n",
      "Train loss 0.05035607820307767\n",
      "Valid loss 0.06285008575235095\n",
      "#################\n",
      "###Epoch: 78\n",
      "Train loss 0.05020501050684187\n",
      "Valid loss 0.06305967643857002\n",
      "#################\n",
      "###Epoch: 79\n",
      "Train loss 0.050028152487896105\n",
      "Valid loss 0.06329078280500003\n",
      "#################\n",
      "###Epoch: 80\n",
      "Train loss 0.04989719335679655\n",
      "Valid loss 0.06518344953656197\n",
      "#################\n",
      "###Epoch: 81\n",
      "Train loss 0.05015543572328709\n",
      "Valid loss 0.0662174757037844\n",
      "#################\n",
      "###Epoch: 82\n",
      "Train loss 0.05121046263310644\n",
      "Valid loss 0.06324279627629689\n",
      "#################\n",
      "###Epoch: 83\n",
      "Train loss 0.04896467855131185\n",
      "Valid loss 0.06405948208911079\n",
      "#################\n",
      "###Epoch: 84\n",
      "Train loss 0.04878869335408564\n",
      "Valid loss 0.0633891091815063\n",
      "#################\n",
      "###Epoch: 85\n",
      "Train loss 0.04804356451387758\n",
      "Valid loss 0.06300729406731469\n",
      "#################\n",
      "###Epoch: 86\n",
      "Train loss 0.04779295540518231\n",
      "Valid loss 0.06251482559101922\n",
      "#################\n",
      "###Epoch: 87\n",
      "Train loss 0.047744240473817895\n",
      "Valid loss 0.06300389926348414\n",
      "#################\n",
      "###Epoch: 88\n",
      "Train loss 0.04855190107115993\n",
      "Valid loss 0.06379615728344236\n",
      "#################\n",
      "###Epoch: 89\n",
      "Train loss 0.0479981005191803\n",
      "Valid loss 0.06242739941392626\n",
      "#################\n",
      "###Epoch: 90\n",
      "Train loss 0.04739841667038423\n",
      "Valid loss 0.06206502126795905\n",
      "#################\n",
      "###Epoch: 91\n",
      "Train loss 0.04623823364575704\n",
      "Valid loss 0.06258579449994224\n",
      "#################\n",
      "###Epoch: 92\n",
      "Train loss 0.04603869688731653\n",
      "Valid loss 0.06212683022022247\n",
      "#################\n",
      "###Epoch: 93\n",
      "Train loss 0.04571255075710791\n",
      "Valid loss 0.06216997067843165\n",
      "#################\n",
      "###Epoch: 94\n",
      "Train loss 0.045803318834967084\n",
      "Valid loss 0.06218861948166575\n",
      "#################\n",
      "###Epoch: 95\n",
      "Train loss 0.04612940729216293\n",
      "Valid loss 0.06294678045170647\n",
      "#################\n",
      "###Epoch: 96\n",
      "Train loss 0.04494138627692505\n",
      "Valid loss 0.06143911076443536\n",
      "#################\n",
      "###Epoch: 97\n",
      "Train loss 0.04491909165625219\n",
      "Valid loss 0.06287909565227372\n",
      "#################\n",
      "###Epoch: 98\n",
      "Train loss 0.045082084006733365\n",
      "Valid loss 0.06373844455395426\n",
      "#################\n",
      "###Epoch: 99\n",
      "Train loss 0.045087083208340185\n",
      "Valid loss 0.06307358507599149\n",
      "#################\n",
      "###Epoch: 0\n",
      "Train loss 0.24628792923909645\n",
      "Valid loss 0.19440298208168574\n",
      "#################\n",
      "###Epoch: 1\n",
      "Train loss 0.17894087290322339\n",
      "Valid loss 0.1616734883614949\n",
      "#################\n",
      "###Epoch: 2\n",
      "Train loss 0.15668098049031365\n",
      "Valid loss 0.14686142333916255\n",
      "#################\n",
      "###Epoch: 3\n",
      "Train loss 0.1425752620454188\n",
      "Valid loss 0.1302302713905062\n",
      "#################\n",
      "###Epoch: 4\n",
      "Train loss 0.13317172477642694\n",
      "Valid loss 0.12351210308926446\n",
      "#################\n",
      "###Epoch: 5\n",
      "Train loss 0.1257621803217464\n",
      "Valid loss 0.11962318952594485\n",
      "#################\n",
      "###Epoch: 6\n",
      "Train loss 0.12290803756978777\n",
      "Valid loss 0.11723704529660088\n",
      "#################\n",
      "###Epoch: 7\n",
      "Train loss 0.11974701671688645\n",
      "Valid loss 0.11568311389003481\n",
      "#################\n",
      "###Epoch: 8\n",
      "Train loss 0.1156622204515669\n",
      "Valid loss 0.11053484678268433\n",
      "#################\n",
      "###Epoch: 9\n",
      "Train loss 0.1129664362028793\n",
      "Valid loss 0.11031376996210643\n",
      "#################\n",
      "###Epoch: 10\n",
      "Train loss 0.10899644814155719\n",
      "Valid loss 0.10533693219934191\n",
      "#################\n",
      "###Epoch: 11\n",
      "Train loss 0.10542795365607296\n",
      "Valid loss 0.10225567115204674\n",
      "#################\n",
      "###Epoch: 12\n",
      "Train loss 0.1035281781245161\n",
      "Valid loss 0.0994831770658493\n",
      "#################\n",
      "###Epoch: 13\n",
      "Train loss 0.10081510742505391\n",
      "Valid loss 0.0967445405466216\n",
      "#################\n",
      "###Epoch: 14\n",
      "Train loss 0.09939612199862798\n",
      "Valid loss 0.0955927020737103\n",
      "#################\n",
      "###Epoch: 15\n",
      "Train loss 0.0965805263430984\n",
      "Valid loss 0.09299069004399436\n",
      "#################\n",
      "###Epoch: 16\n",
      "Train loss 0.0933992479134489\n",
      "Valid loss 0.09059186066899981\n",
      "#################\n",
      "###Epoch: 17\n",
      "Train loss 0.09205231401655409\n",
      "Valid loss 0.08960874165807452\n",
      "#################\n",
      "###Epoch: 18\n",
      "Train loss 0.09025627374649048\n",
      "Valid loss 0.0895751416683197\n",
      "#################\n",
      "###Epoch: 19\n",
      "Train loss 0.08869743981847057\n",
      "Valid loss 0.08749864143984658\n",
      "#################\n",
      "###Epoch: 20\n",
      "Train loss 0.08648090064525604\n",
      "Valid loss 0.08409413801772254\n",
      "#################\n",
      "###Epoch: 21\n",
      "Train loss 0.08400817187847914\n",
      "Valid loss 0.08217042897428785\n",
      "#################\n",
      "###Epoch: 22\n",
      "Train loss 0.08284518177862521\n",
      "Valid loss 0.08149607053824834\n",
      "#################\n",
      "###Epoch: 23\n",
      "Train loss 0.08066128139142636\n",
      "Valid loss 0.0796475453036172\n",
      "#################\n",
      "###Epoch: 24\n",
      "Train loss 0.0788041016569844\n",
      "Valid loss 0.07912031667573112\n",
      "#################\n",
      "###Epoch: 25\n",
      "Train loss 0.0770099088549614\n",
      "Valid loss 0.07713080836193902\n",
      "#################\n",
      "###Epoch: 26\n",
      "Train loss 0.0763237644676809\n",
      "Valid loss 0.07622945521559034\n",
      "#################\n",
      "###Epoch: 27\n",
      "Train loss 0.07527094334363937\n",
      "Valid loss 0.07449251732655934\n",
      "#################\n",
      "###Epoch: 28\n",
      "Train loss 0.0739481540189849\n",
      "Valid loss 0.07433179233755384\n",
      "#################\n",
      "###Epoch: 29\n",
      "Train loss 0.07309758428622175\n",
      "Valid loss 0.07521199009248189\n",
      "#################\n",
      "###Epoch: 30\n",
      "Train loss 0.07172863889071676\n",
      "Valid loss 0.07449755924088615\n",
      "#################\n",
      "###Epoch: 31\n",
      "Train loss 0.07048897031280729\n",
      "Valid loss 0.0727958796279771\n",
      "#################\n",
      "###Epoch: 32\n",
      "Train loss 0.06998302884123943\n",
      "Valid loss 0.07197078636714391\n",
      "#################\n",
      "###Epoch: 33\n",
      "Train loss 0.06861512583714945\n",
      "Valid loss 0.07198224536010198\n",
      "#################\n",
      "###Epoch: 34\n",
      "Train loss 0.06856660641453884\n",
      "Valid loss 0.07061010279825755\n",
      "#################\n",
      "###Epoch: 35\n",
      "Train loss 0.0688732738296191\n",
      "Valid loss 0.07134811047996793\n",
      "#################\n",
      "###Epoch: 36\n",
      "Train loss 0.06856874086790615\n",
      "Valid loss 0.07125652900763921\n",
      "#################\n",
      "###Epoch: 37\n",
      "Train loss 0.06625951702396075\n",
      "Valid loss 0.07076212550912585\n",
      "#################\n",
      "###Epoch: 38\n",
      "Train loss 0.06579269137647417\n",
      "Valid loss 0.07034433526652199\n",
      "#################\n",
      "###Epoch: 39\n",
      "Train loss 0.06728435694067567\n",
      "Valid loss 0.07104649181876864\n",
      "#################\n",
      "###Epoch: 40\n",
      "Train loss 0.06523734758849498\n",
      "Valid loss 0.0690532885491848\n",
      "#################\n",
      "###Epoch: 41\n",
      "Train loss 0.06388577805073173\n",
      "Valid loss 0.0688491580741746\n",
      "#################\n",
      "###Epoch: 42\n",
      "Train loss 0.06293572554433788\n",
      "Valid loss 0.07114198484591075\n",
      "#################\n",
      "###Epoch: 43\n",
      "Train loss 0.06235261661586938\n",
      "Valid loss 0.06997977197170258\n",
      "#################\n",
      "###Epoch: 44\n",
      "Train loss 0.061284617279414776\n",
      "Valid loss 0.06787138645138059\n",
      "#################\n",
      "###Epoch: 45\n",
      "Train loss 0.06203607731947192\n",
      "Valid loss 0.06854537022965294\n",
      "#################\n",
      "###Epoch: 46\n",
      "Train loss 0.06086185133015668\n",
      "Valid loss 0.06742128250854355\n",
      "#################\n",
      "###Epoch: 47\n",
      "Train loss 0.06000092156507351\n",
      "Valid loss 0.06822761254651206\n",
      "#################\n",
      "###Epoch: 48\n",
      "Train loss 0.05934822918088348\n",
      "Valid loss 0.06652299953358513\n",
      "#################\n",
      "###Epoch: 49\n",
      "Train loss 0.059312186031429855\n",
      "Valid loss 0.06669572155390467\n",
      "#################\n",
      "###Epoch: 50\n",
      "Train loss 0.0586450545600167\n",
      "Valid loss 0.06740119042141098\n",
      "#################\n",
      "###Epoch: 51\n",
      "Train loss 0.05911073833703995\n",
      "Valid loss 0.06796105259231158\n",
      "#################\n",
      "###Epoch: 52\n",
      "Train loss 0.05863241920316661\n",
      "Valid loss 0.06603862717747688\n",
      "#################\n",
      "###Epoch: 53\n",
      "Train loss 0.05816524241257597\n",
      "Valid loss 0.06579570631895747\n",
      "#################\n",
      "###Epoch: 54\n",
      "Train loss 0.058522933887110815\n",
      "Valid loss 0.06601756545049804\n",
      "#################\n",
      "###Epoch: 55\n",
      "Train loss 0.05717210885551241\n",
      "Valid loss 0.06846843287348747\n",
      "#################\n",
      "###Epoch: 56\n",
      "Train loss 0.05646646050391374\n",
      "Valid loss 0.06702435123068946\n",
      "#################\n",
      "###Epoch: 57\n",
      "Train loss 0.05572089249337161\n",
      "Valid loss 0.06633468770555087\n",
      "#################\n",
      "###Epoch: 58\n",
      "Train loss 0.05580330571090734\n",
      "Valid loss 0.0663762055337429\n",
      "#################\n",
      "###Epoch: 59\n",
      "Train loss 0.054992982634791625\n",
      "Valid loss 0.06524482209767614\n",
      "#################\n",
      "###Epoch: 60\n",
      "Train loss 0.054904728989910195\n",
      "Valid loss 0.06484349870256015\n",
      "#################\n",
      "###Epoch: 61\n",
      "Train loss 0.05432142855392562\n",
      "Valid loss 0.06539462719644819\n",
      "#################\n",
      "###Epoch: 62\n",
      "Train loss 0.05404554128094956\n",
      "Valid loss 0.06523317258272852\n",
      "#################\n",
      "###Epoch: 63\n",
      "Train loss 0.05405118647548887\n",
      "Valid loss 0.06689384366784777\n",
      "#################\n",
      "###Epoch: 64\n",
      "Train loss 0.05408059129560435\n",
      "Valid loss 0.06569967897874969\n",
      "#################\n",
      "###Epoch: 65\n",
      "Train loss 0.05333266710793531\n",
      "Valid loss 0.06496096562061991\n",
      "#################\n",
      "###Epoch: 66\n",
      "Train loss 0.05292297692762481\n",
      "Valid loss 0.06491515998329435\n",
      "#################\n",
      "###Epoch: 67\n",
      "Train loss 0.05269623437413463\n",
      "Valid loss 0.06590167592678751\n",
      "#################\n",
      "###Epoch: 68\n",
      "Train loss 0.05217096540662977\n",
      "Valid loss 0.06517490265624863\n",
      "#################\n",
      "###Epoch: 69\n",
      "Train loss 0.051806900098367976\n",
      "Valid loss 0.0645688186798777\n",
      "#################\n",
      "###Epoch: 70\n",
      "Train loss 0.051433141032854714\n",
      "Valid loss 0.06507626814501626\n",
      "#################\n",
      "###Epoch: 71\n",
      "Train loss 0.0520814566148652\n",
      "Valid loss 0.06601771020463534\n",
      "#################\n",
      "###Epoch: 72\n",
      "Train loss 0.05200583694709672\n",
      "Valid loss 0.06965701654553413\n",
      "#################\n",
      "###Epoch: 73\n",
      "Train loss 0.05069352603620953\n",
      "Valid loss 0.06530971452593803\n",
      "#################\n",
      "###Epoch: 74\n",
      "Train loss 0.050011556733537604\n",
      "Valid loss 0.06557842131171908\n",
      "#################\n",
      "###Epoch: 75\n",
      "Train loss 0.04986824357399234\n",
      "Valid loss 0.06437052999223981\n",
      "#################\n",
      "###Epoch: 76\n",
      "Train loss 0.049898992809984416\n",
      "Valid loss 0.06704524265868324\n",
      "#################\n",
      "###Epoch: 77\n",
      "Train loss 0.05004781815740797\n",
      "Valid loss 0.06587548128196172\n",
      "#################\n",
      "###Epoch: 78\n",
      "Train loss 0.048818958440312636\n",
      "Valid loss 0.06382290539996964\n",
      "#################\n",
      "###Epoch: 79\n",
      "Train loss 0.04895706460983665\n",
      "Valid loss 0.06435712373682431\n",
      "#################\n",
      "###Epoch: 80\n",
      "Train loss 0.04881214778180475\n",
      "Valid loss 0.06490547795380865\n",
      "#################\n",
      "###Epoch: 81\n",
      "Train loss 0.04845409412626867\n",
      "Valid loss 0.06461973754423005\n",
      "#################\n",
      "###Epoch: 82\n",
      "Train loss 0.04865629170779829\n",
      "Valid loss 0.06398577455963407\n",
      "#################\n",
      "###Epoch: 83\n",
      "Train loss 0.04795430283303614\n",
      "Valid loss 0.06451788225344249\n",
      "#################\n",
      "###Epoch: 84\n",
      "Train loss 0.04764783023684113\n",
      "Valid loss 0.0668283658368247\n",
      "#################\n",
      "###Epoch: 85\n",
      "Train loss 0.04849063605070114\n",
      "Valid loss 0.06517349715743746\n",
      "#################\n",
      "###Epoch: 86\n",
      "Train loss 0.04762875688848672\n",
      "Valid loss 0.06467867961951665\n",
      "#################\n",
      "###Epoch: 87\n",
      "Train loss 0.04736513258130462\n",
      "Valid loss 0.06377662026456424\n",
      "#################\n",
      "###Epoch: 88\n",
      "Train loss 0.04651267095296471\n",
      "Valid loss 0.06343156365411622\n",
      "#################\n",
      "###Epoch: 89\n",
      "Train loss 0.04633812275197771\n",
      "Valid loss 0.06494127213954926\n",
      "#################\n",
      "###Epoch: 90\n",
      "Train loss 0.046176487372981176\n",
      "Valid loss 0.06414966232010297\n",
      "#################\n",
      "###Epoch: 91\n",
      "Train loss 0.04642046008397032\n",
      "Valid loss 0.06432772693889481\n",
      "#################\n",
      "###Epoch: 92\n",
      "Train loss 0.04560598206740839\n",
      "Valid loss 0.06470945317830358\n",
      "#################\n",
      "###Epoch: 93\n",
      "Train loss 0.045895183113990004\n",
      "Valid loss 0.06546307195510183\n",
      "#################\n",
      "###Epoch: 94\n",
      "Train loss 0.045364622992497904\n",
      "Valid loss 0.06368402710982732\n",
      "#################\n",
      "###Epoch: 95\n",
      "Train loss 0.044947186277972326\n",
      "Valid loss 0.06537907836692673\n",
      "#################\n",
      "###Epoch: 96\n",
      "Train loss 0.04517836344462854\n",
      "Valid loss 0.06356103505407061\n",
      "#################\n",
      "###Epoch: 97\n",
      "Train loss 0.04463921432141905\n",
      "Valid loss 0.06474122085741588\n",
      "#################\n",
      "###Epoch: 98\n",
      "Train loss 0.04431866095573814\n",
      "Valid loss 0.06369967386126518\n",
      "#################\n",
      "###Epoch: 99\n",
      "Train loss 0.04451281894688253\n",
      "Valid loss 0.06553839253527778\n"
     ]
    }
   ],
   "source": [
    "splits = KFold(n_splits=config.n_split, shuffle=True, random_state=config.seed).split(train_inputs)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(splits):\n",
    "    train_dataset = TensorDataset(train_inputs[train_idx], train_adj[train_idx], train_labels[train_idx])\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=8)\n",
    "    \n",
    "    valid_dataset = TensorDataset(train_inputs[val_idx], train_adj[val_idx], train_labels[val_idx])\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=config.batch_size, shuffle=False, num_workers=8)\n",
    "    \n",
    "    train_losses, eval_losses = run(fold, train_loader, valid_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## required nbformat\n",
    "fig = px.line(\n",
    "    pd.DataFrame([train_losses, eval_losses], index=['loss', 'val_loss']).T, \n",
    "    y=['loss', 'val_loss'], \n",
    "    labels={'index': 'epoch', 'value': 'Mean Squared Error'}, \n",
    "    title='Training History')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "public_df = test.query(\"seq_length == 107\").copy()\n",
    "private_df = test.query(\"seq_length == 130\").copy()\n",
    "\n",
    "public_inputs, public_adj = preprocess_inputs(public_df)\n",
    "private_inputs, private_adj = preprocess_inputs(private_df)\n",
    "\n",
    "public_inputs = torch.tensor(public_inputs, dtype=torch.long)\n",
    "private_inputs = torch.tensor(private_inputs, dtype=torch.long)\n",
    "public_adj = torch.tensor(public_adj, dtype=torch.long)\n",
    "private_adj = torch.tensor(private_adj, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_short = Net(seq_len=107, pred_len=107, K=config.K, aggregator=config.gcn_agg)\n",
    "model_long = Net(seq_len=130, pred_len=130, K=config.K, aggregator=config.gcn_agg)\n",
    "\n",
    "list_public_preds = []\n",
    "list_private_preds = []\n",
    "for fold in range(config.n_split):\n",
    "    model_short.load_state_dict(torch.load(f'{config.pretrain_dir}/005_gcn_gru_{fold}.pt'))\n",
    "    model_long.load_state_dict(torch.load(f'{config.pretrain_dir}/005_gcn_gru_{fold}.pt'))\n",
    "    model_short.cuda()\n",
    "    model_long.cuda()\n",
    "    model_short.eval()\n",
    "    model_long.eval()\n",
    "\n",
    "    public_preds = model_short(public_inputs.cuda(), public_adj.cuda())\n",
    "    private_preds = model_long(private_inputs.cuda(), private_adj.cuda())\n",
    "    public_preds = public_preds.cpu().detach().numpy()\n",
    "    private_preds = private_preds.cpu().detach().numpy()\n",
    "    \n",
    "    list_public_preds.append(public_preds)\n",
    "    list_private_preds.append(private_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get predict results by averaging results in 5-folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "public_preds = np.mean(list_public_preds, axis=0)\n",
    "private_preds = np.mean(list_private_preds, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_ls = []\n",
    "\n",
    "for df, preds in [(public_df, public_preds), (private_df, private_preds)]:\n",
    "    for i, uid in enumerate(df.id):\n",
    "        single_pred = preds[i]\n",
    "\n",
    "        single_df = pd.DataFrame(single_pred, columns=pred_cols)\n",
    "        single_df['id_seqpos'] = [f'{uid}_{x}' for x in range(single_df.shape[0])]\n",
    "\n",
    "        preds_ls.append(single_df)\n",
    "\n",
    "preds_df = pd.concat(preds_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = sample_df[['id_seqpos']].merge(preds_df, on=['id_seqpos'])\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_seqpos</th>\n",
       "      <th>reactivity</th>\n",
       "      <th>deg_Mg_pH10</th>\n",
       "      <th>deg_pH10</th>\n",
       "      <th>deg_Mg_50C</th>\n",
       "      <th>deg_50C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_00073f8be_0</td>\n",
       "      <td>0.761022</td>\n",
       "      <td>0.661857</td>\n",
       "      <td>1.999782</td>\n",
       "      <td>0.550934</td>\n",
       "      <td>0.781211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_00073f8be_1</td>\n",
       "      <td>2.585828</td>\n",
       "      <td>3.305934</td>\n",
       "      <td>4.814991</td>\n",
       "      <td>3.379502</td>\n",
       "      <td>3.024239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_00073f8be_2</td>\n",
       "      <td>1.547143</td>\n",
       "      <td>0.471197</td>\n",
       "      <td>0.574019</td>\n",
       "      <td>0.577931</td>\n",
       "      <td>0.695757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_00073f8be_3</td>\n",
       "      <td>1.399923</td>\n",
       "      <td>1.165978</td>\n",
       "      <td>1.269586</td>\n",
       "      <td>1.797334</td>\n",
       "      <td>1.704910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_00073f8be_4</td>\n",
       "      <td>0.860538</td>\n",
       "      <td>0.539656</td>\n",
       "      <td>0.573501</td>\n",
       "      <td>0.852607</td>\n",
       "      <td>0.861463</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id_seqpos  reactivity  deg_Mg_pH10  deg_pH10  deg_Mg_50C   deg_50C\n",
       "0  id_00073f8be_0    0.761022     0.661857  1.999782    0.550934  0.781211\n",
       "1  id_00073f8be_1    2.585828     3.305934  4.814991    3.379502  3.024239\n",
       "2  id_00073f8be_2    1.547143     0.471197  0.574019    0.577931  0.695757\n",
       "3  id_00073f8be_3    1.399923     1.165978  1.269586    1.797334  1.704910\n",
       "4  id_00073f8be_4    0.860538     0.539656  0.573501    0.852607  0.861463"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
