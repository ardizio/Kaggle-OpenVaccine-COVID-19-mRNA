{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceLoader(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe_path, signal_noise_cutoff, test_set=None):\n",
    "        super().__init__()\n",
    "        self.df = pd.read_json(dataframe_path)\n",
    "        deg_cols = ['reactivity', 'deg_Mg_pH10', 'deg_Mg_50C']\n",
    "        \n",
    "        self.is_test = test_set is not None or deg_cols[0] not in self.df.columns\n",
    "        if self.is_test:\n",
    "            self.df = self.df.query((\"seq_length == 107\" if test_set == 'public' else \"seq_length == 130\"))\n",
    "            self.y = None\n",
    "        else:\n",
    "            self.df = self.df[self.df.signal_to_noise >= signal_noise_cutoff]\n",
    "            self.y = np.stack([np.stack(self.df[col].values) for col in deg_cols], axis=-1)\n",
    "\n",
    "        self.sample_ids = self.df['id'].values\n",
    "        self.X = np.stack(self.df['train_tensor'].values)\n",
    "        self.id_to_bp_mat_map = self.load_bp_mats()\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        x = torch.tensor(self.X[index, :, :], dtype=torch.float32)\n",
    "        seq_adj = self.get_sequence_adjacency(x.size()[0])\n",
    "        bp_adj = self.get_base_pair_adjacency(self.sample_ids[index])\n",
    "\n",
    "        if self.is_test:\n",
    "            sample_id = self.sample_ids[index]\n",
    "            return sample_id, x, seq_adj, bp_adj\n",
    "\n",
    "        targets = torch.tensor(self.y[index, :, :], dtype=torch.float32)\n",
    "        return x, targets, seq_adj, bp_adj\n",
    "\n",
    "    @staticmethod\n",
    "    def get_sequence_adjacency(size):\n",
    "        r_shift = np.pad(np.identity(size), ((0, 0), (1, 0)), mode='constant')[:, :-1]\n",
    "        l_shift = np.pad(np.identity(size), ((0, 0), (0, 1)), mode='constant')[:, 1:]\n",
    "        return torch.tensor(r_shift + l_shift, dtype=torch.float32)\n",
    "\n",
    "    def get_base_pair_adjacency(self, sample_id):\n",
    "        return self.id_to_bp_mat_map[sample_id]\n",
    "\n",
    "    def load_bp_mats(self):\n",
    "        res = {}\n",
    "        for sid in self.sample_ids:\n",
    "            res[sid] = torch.tensor(np.load('dataset/bpps/' + sid + '.npy'), dtype=torch.float32)\n",
    "        return res\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return self.df.shape[0]\n",
    "\n",
    "\n",
    "def dataset_loader(df_path, batch_size, signal_noise_cutoff=-99.0, test_set=None):\n",
    "    dataset = SequenceLoader(df_path, signal_noise_cutoff, test_set=test_set)\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=(test_set is None),\n",
    "        num_workers=4\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates an encoding to capture a node's position (just like transformers).\n",
    "# Used as part of the GraphBlock.\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.2, max_len=300):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x, offset=None):\n",
    "        x = x + self.pe[:x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "    \n",
    "\n",
    "# Provides info about the entire graph to every node.\n",
    "class GraphBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, mlp_size, dropout=0.2):\n",
    "        super(GraphBlock, self).__init__()\n",
    "        self.pos_encoder = PositionalEncoding(mlp_size)\n",
    "        self.graph_layer = nn.Linear(mlp_size, mlp_size)\n",
    "        self.layer_norm = torch.nn.LayerNorm(mlp_size)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        graph_node_emb = torch.tanh(self.graph_layer(x))\n",
    "        graph_node_emb = self.dropout(graph_node_emb)\n",
    "        graph_emb = torch.mean(graph_node_emb, 1)\n",
    "        graph_emb = graph_emb.reshape((graph_emb.size()[0], 1, graph_emb.size()[1]))\n",
    "\n",
    "        pos_enc = self.pos_encoder(x)\n",
    "        return self.dropout(self.layer_norm(pos_enc + graph_emb))\n",
    "\n",
    "\n",
    "class ConvAttnBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, mlp_size, adj_conv_channels=4, dropout=0.2):\n",
    "        super(ConvAttnBlock, self).__init__()\n",
    "        self.adj_conv_channels = adj_conv_channels\n",
    "        self.neighbor_layer = nn.Linear(mlp_size, mlp_size)\n",
    "        self.result_layer = nn.Linear(mlp_size, mlp_size)\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels=1, out_channels=adj_conv_channels, \n",
    "                                     kernel_size=(7), padding=3)\n",
    "        self.conv2 = torch.nn.Conv2d(in_channels=adj_conv_channels, \n",
    "                                     out_channels=adj_conv_channels, kernel_size=(17), padding=8)\n",
    "        self.layer_norm = torch.nn.LayerNorm(mlp_size)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, bp_adj):\n",
    "        batch = bp_adj.size(0)\n",
    "        d1 = bp_adj.size(1)\n",
    "        d2 = bp_adj.size(2)\n",
    "\n",
    "        bp_adj = self.conv1(bp_adj.reshape((batch, 1, d1, d2)))\n",
    "        bp_adj = self.conv2(bp_adj)\n",
    "        bp_adj = torch.mean(bp_adj, 1)[:, :d1, :d2]\n",
    "        neighbor_emb = torch.tanh(self.neighbor_layer(x))\n",
    "        neighbor_emb = self.dropout(neighbor_emb)\n",
    "        neighbor_sum = torch.matmul(bp_adj, neighbor_emb)\n",
    "        cat = x + neighbor_sum\n",
    "        out = torch.tanh(self.result_layer(cat))\n",
    "        out = self.layer_norm(out)\n",
    "        return self.dropout(out), bp_adj\n",
    "\n",
    "\n",
    "# Combines 2 rounds of conv attention on the sequence and BPP matrices.\n",
    "# Includes data from 1 graph block and a skip connection.\n",
    "class NeighborAttnStage(nn.Module):\n",
    "    def __init__(self, mlp_size, dropout=0.2):\n",
    "        super(NeighborAttnStage, self).__init__()\n",
    "        self.graph_block = GraphBlock(mlp_size)\n",
    "        self.sequence_block1 = ConvAttnBlock(mlp_size)\n",
    "        self.sequence_block2 = ConvAttnBlock(mlp_size)\n",
    "        self.base_pair_block1 = ConvAttnBlock(mlp_size)\n",
    "        self.base_pair_block2 = ConvAttnBlock(mlp_size)\n",
    "        \n",
    "    def forward(self, x_in, seq_adj, bp_adj):\n",
    "        x_bp1, _ = self.base_pair_block1(x_in, bp_adj)\n",
    "        x_seq1, _ = self.sequence_block1(x_in, seq_adj)\n",
    "        x = self.graph_block(x_in) + x_bp1 + x_seq1\n",
    "\n",
    "        x_bp2, _ = self.base_pair_block2(x, bp_adj)\n",
    "        x_seq2, _ = self.sequence_block2(x, seq_adj)\n",
    "        return x_bp2 + x_seq2 + x_in\n",
    "\n",
    "\n",
    "class NeighborhoodAttentionModel(nn.Module):\n",
    "\n",
    "    def __init__(self, mlp_size, dropout=0.2):\n",
    "        super(NeighborhoodAttentionModel, self).__init__()\n",
    "        self.input_fc = nn.Linear(14, mlp_size)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.conv_attn_stage1 = NeighborAttnStage(mlp_size)\n",
    "        self.conv_attn_stage2 = NeighborAttnStage(mlp_size)\n",
    "        self.conv_attn_stage3 = NeighborAttnStage(mlp_size)\n",
    "        self.output_fc1 = nn.Linear(mlp_size, mlp_size)\n",
    "        self.output_fc2 = nn.Linear(mlp_size, 3)\n",
    "\n",
    "    def forward(self, x, seq_adj, bp_adj):\n",
    "        x = torch.tanh(self.input_fc(x))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.conv_attn_stage1(x, seq_adj, bp_adj)\n",
    "        x = self.conv_attn_stage2(x, seq_adj, bp_adj)\n",
    "        x = self.conv_attn_stage3(x, seq_adj, bp_adj)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "        x = torch.tanh(self.output_fc1(x))\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        return self.output_fc2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uninteresting code used to keep track of average loss over an epoch\n",
    "class Averager:\n",
    "    def __init__(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    def send(self, value):\n",
    "        self.current_total += value\n",
    "        self.iterations += 1\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        if self.iterations == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1.0 * self.current_total / self.iterations\n",
    "\n",
    "    @property\n",
    "    def time(self):\n",
    "        return time.time() - self.start_time\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0\n",
    "        self.start_time = time.time()\n",
    "\n",
    "\n",
    "# Pytorch MCRMSE Losss\n",
    "# [Link to Kernel]\n",
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, yhat, y):\n",
    "        loss = torch.sqrt(self.mse(yhat, y) + self.eps)\n",
    "        return loss\n",
    "\n",
    "\n",
    "class MCRMSELoss(nn.Module):\n",
    "    def __init__(self, num_scored=3):\n",
    "        super().__init__()\n",
    "        self.rmse = RMSELoss()\n",
    "        self.num_scored = num_scored\n",
    "\n",
    "    def forward(self, yhat, y):\n",
    "        score = 0\n",
    "        for i in range(self.num_scored):\n",
    "            score += self.rmse(yhat[:, :, i], y[:, :, i]) / self.num_scored\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Get Device (CPU or GPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "# Hyperparameters\n",
    "epochs = 100\n",
    "batch_size = 8\n",
    "node_emb_size = 512\n",
    "lr = 0.0001\n",
    "lr_drop_epochs = 45\n",
    "lr_gamma = 0.1\n",
    "criterion = MCRMSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Train/Val Data Loaders\n",
    "prepd_train_data = 'dataset/train_1.json'\n",
    "prepd_val_data = 'dataset/val_1.json'\n",
    "train_loader = dataset_loader(prepd_train_data, batch_size=batch_size, signal_noise_cutoff=0.6)\n",
    "val_loader = dataset_loader(prepd_val_data, batch_size=batch_size, signal_noise_cutoff=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x00000240F0D8B810>\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x00000240EECCC8D0>\n"
     ]
    }
   ],
   "source": [
    "print(train_loader)\n",
    "print(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model = NeighborhoodAttentionModel(node_emb_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeighborhoodAttentionModel(\n",
      "  (input_fc): Linear(in_features=14, out_features=512, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (conv_attn_stage1): NeighborAttnStage(\n",
      "    (graph_block): GraphBlock(\n",
      "      (pos_encoder): PositionalEncoding(\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (graph_layer): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "    (sequence_block1): ConvAttnBlock(\n",
      "      (neighbor_layer): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (result_layer): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (conv1): Conv2d(1, 4, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
      "      (conv2): Conv2d(4, 4, kernel_size=(17, 17), stride=(1, 1), padding=(8, 8))\n",
      "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "    (sequence_block2): ConvAttnBlock(\n",
      "      (neighbor_layer): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (result_layer): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (conv1): Conv2d(1, 4, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
      "      (conv2): Conv2d(4, 4, kernel_size=(17, 17), stride=(1, 1), padding=(8, 8))\n",
      "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "    (base_pair_block1): ConvAttnBlock(\n",
      "      (neighbor_layer): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (result_layer): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (conv1): Conv2d(1, 4, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
      "      (conv2): Conv2d(4, 4, kernel_size=(17, 17), stride=(1, 1), padding=(8, 8))\n",
      "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "    (base_pair_block2): ConvAttnBlock(\n",
      "      (neighbor_layer): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (result_layer): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (conv1): Conv2d(1, 4, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
      "      (conv2): Conv2d(4, 4, kernel_size=(17, 17), stride=(1, 1), padding=(8, 8))\n",
      "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (conv_attn_stage2): NeighborAttnStage(\n",
      "    (graph_block): GraphBlock(\n",
      "      (pos_encoder): PositionalEncoding(\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (graph_layer): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "    (sequence_block1): ConvAttnBlock(\n",
      "      (neighbor_layer): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (result_layer): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (conv1): Conv2d(1, 4, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
      "      (conv2): Conv2d(4, 4, kernel_size=(17, 17), stride=(1, 1), padding=(8, 8))\n",
      "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "    (sequence_block2): ConvAttnBlock(\n",
      "      (neighbor_layer): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (result_layer): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (conv1): Conv2d(1, 4, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
      "      (conv2): Conv2d(4, 4, kernel_size=(17, 17), stride=(1, 1), padding=(8, 8))\n",
      "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "    (base_pair_block1): ConvAttnBlock(\n",
      "      (neighbor_layer): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (result_layer): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (conv1): Conv2d(1, 4, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
      "      (conv2): Conv2d(4, 4, kernel_size=(17, 17), stride=(1, 1), padding=(8, 8))\n",
      "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "    (base_pair_block2): ConvAttnBlock(\n",
      "      (neighbor_layer): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (result_layer): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (conv1): Conv2d(1, 4, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
      "      (conv2): Conv2d(4, 4, kernel_size=(17, 17), stride=(1, 1), padding=(8, 8))\n",
      "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (conv_attn_stage3): NeighborAttnStage(\n",
      "    (graph_block): GraphBlock(\n",
      "      (pos_encoder): PositionalEncoding(\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (graph_layer): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "    (sequence_block1): ConvAttnBlock(\n",
      "      (neighbor_layer): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (result_layer): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (conv1): Conv2d(1, 4, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
      "      (conv2): Conv2d(4, 4, kernel_size=(17, 17), stride=(1, 1), padding=(8, 8))\n",
      "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "    (sequence_block2): ConvAttnBlock(\n",
      "      (neighbor_layer): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (result_layer): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (conv1): Conv2d(1, 4, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
      "      (conv2): Conv2d(4, 4, kernel_size=(17, 17), stride=(1, 1), padding=(8, 8))\n",
      "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "    (base_pair_block1): ConvAttnBlock(\n",
      "      (neighbor_layer): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (result_layer): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (conv1): Conv2d(1, 4, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
      "      (conv2): Conv2d(4, 4, kernel_size=(17, 17), stride=(1, 1), padding=(8, 8))\n",
      "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "    (base_pair_block2): ConvAttnBlock(\n",
      "      (neighbor_layer): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (result_layer): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (conv1): Conv2d(1, 4, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
      "      (conv2): Conv2d(4, 4, kernel_size=(17, 17), stride=(1, 1), padding=(8, 8))\n",
      "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (output_fc1): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (output_fc2): Linear(in_features=512, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer & Scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, lr_drop_epochs, gamma=lr_gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# Objects for keeping track of loss over epochs.\n",
    "epoch_loss_hist = Averager()\n",
    "val_loss_hist = Averager()\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    epoch_loss_hist.reset()\n",
    "    \n",
    "    print(epoch)\n",
    "\n",
    "    # Training\n",
    "    for sequences, targets, seq_adj_matrix, bp_adj_matrix in train_loader:\n",
    "        print(\"for init\")\n",
    "        print(sequences)\n",
    "        print(targets)\n",
    "        print(seq_adj_matrix)\n",
    "        print(bp_adj_matrix)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(sequences.to(device), seq_adj_matrix.to(device), bp_adj_matrix.to(device))\n",
    "        loss = criterion(pred[:, :targets.size()[1], :], targets.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss_hist.send(loss.item())\n",
    "        \n",
    "    # Validation\n",
    "    with torch.no_grad():\n",
    "        print(\"NO GRAD\")\n",
    "        model.eval()\n",
    "        val_loss_hist.reset()\n",
    "\n",
    "        for sequences, targets, seq_adj_matrix, bp_adj_matrix in val_loader:\n",
    "            print(\"for NO GRAD\")\n",
    "            print(sequences)\n",
    "            print(targets)\n",
    "            print(seq_adj_matrix)\n",
    "            print(bp_adj_matrix)\n",
    "            pred = model(sequences.to(device), seq_adj_matrix.to(device), bp_adj_matrix.to(device))\n",
    "            loss = criterion(pred[:, :targets.size(1), :], targets.to(device))\n",
    "            val_loss_hist.send(loss.item())\n",
    "\n",
    "    print('Epoch:', epoch, 'Train Loss:', epoch_loss_hist.value, 'CV Loss:', val_loss_hist.value)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_submission_df(ids, pred_tensor):\n",
    "    if type(pred_tensor).__module__ != np.__name__:\n",
    "        pred_tensor = pred_tensor.cpu().detach().numpy()\n",
    "    res = []\n",
    "    for i, id in enumerate(ids):\n",
    "        for j, pred in enumerate(pred_tensor[i, :, :]):\n",
    "            res.append([id+'_'+str(j)] + list(pred))\n",
    "    return res\n",
    "\n",
    "\n",
    "def make_pred_file(model, loaders, postfix=''):\n",
    "    res = []\n",
    "    for loader in loaders:\n",
    "        for ids, sequences, seq_adj_matrix, bp_adj_matrix in loader:\n",
    "            test_pred = model(sequences.to(device), seq_adj_matrix.to(device), bp_adj_matrix.to(device))\n",
    "            res += build_submission_df(ids, test_pred)\n",
    "\n",
    "    pred_df = pd.DataFrame(res, columns=['id_seqpos', 'reactivity', 'deg_Mg_pH10', 'deg_Mg_50C'])\n",
    "    pred_df['deg_pH10'] = 0\n",
    "    pred_df['deg_50C'] = 0\n",
    "    pred_df.to_csv('dataset/submission'+postfix+'.csv', index=False)\n",
    "    \n",
    "\n",
    "\n",
    "test_data_path = 'dataset/test_1.json'\n",
    "test_data_loader1 = dataset_loader(test_data_path, test_set='public', batch_size=batch_size)\n",
    "test_data_loader2 = dataset_loader(test_data_path, test_set='private', batch_size=batch_size)\n",
    "make_pred_file(model, [test_data_loader1, test_data_loader2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
